{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1 - Content Based Image Retrieval\n",
    "### Team 8 - Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, glob, math, tqdm, pickle, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import pytesseract\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "import pywt\n",
    "from skimage.feature import local_binary_pattern\n",
    "from scipy.fftpack import dctn\n",
    "\n",
    "# Path to the OCR executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Luis\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts a specific text pattern from a file.\n",
    "def get_text_bbdd(path):\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        line = f.readlines()\n",
    "        \n",
    "    # Loop through each line in the file.\n",
    "    for l in line:\n",
    "        # Check if the line contains a pattern that starts with \"(' and ends with ')\".\n",
    "        if re.search(r\"\\('([^']+)'\", l.split(',')[0]):\n",
    "            # If pattern is found, return the text inside the parentheses.\n",
    "            return re.search(r\"\\('([^']+)'\", l.split(',')[0]).group(1)\n",
    "        else:\n",
    "            # If pattern is not found, return 'Unknown'.\n",
    "            return 'Unknown'\n",
    "\n",
    "# Create a set to store unique names of authors.\n",
    "name_bag = set()\n",
    "\n",
    "for folder in ['BBDD', 'qsd1_w3', 'qsd2_w3']:\n",
    "    # Loop through each .txt file inside the folder.\n",
    "    for text_file in glob.glob(f'data/{folder}/*.txt'):\n",
    "        # Extract the specific text pattern from the file and add it to the set.\n",
    "        name_bag.add(get_text_bbdd(text_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "\n",
    "    # Obtain the painting image removing the background. \n",
    "    # It returns the mask where 1 means painting image and 0 background.\n",
    "    def get_mask(self, img, threshold_area=71000):\n",
    "\n",
    "        # Convert the image to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "     \n",
    "        # Empty mask definition\n",
    "        mask = np.zeros(gray.shape, dtype=np.uint8)\n",
    "\n",
    "        # Applying gaussian blurring and define an intelligent gradient threshold depending on 13x13 boxes\n",
    "        blur = cv2.GaussianBlur(gray, (13,13), 0)\n",
    "        # Threshold based on local pixel neighborhood (11x11 block size)\n",
    "        thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "        # Two pass dilate with horizontal and vertical kernel\n",
    "        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,5))\n",
    "        dilate = cv2.dilate(thresh, horizontal_kernel, iterations=2)\n",
    "        vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,1))\n",
    "        dilate = cv2.dilate(dilate, vertical_kernel, iterations=2)\n",
    "\n",
    "        # Find contours, filter using contour threshold area, and draw rectangle\n",
    "        cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "        # Filtering the found contours by size\n",
    "        counter = 0\n",
    "        areas = []\n",
    "        coordinates = []\n",
    "        for c in cnts:\n",
    "            # Shoelace formula for convex shapes\n",
    "            area = cv2.contourArea(c) \n",
    "            if area > threshold_area:\n",
    "                x,y,w,h = cv2.boundingRect(c) \n",
    "                areas.append((area, (x,y,w,h)))\n",
    "                counter += 1\n",
    "\n",
    "        # Sort areas and positions by area\n",
    "        areas = sorted(areas, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Draw bounding box on mask\n",
    "        for i in range(min(len(areas), 2)):\n",
    "            x,y,w,h = areas[i][1]\n",
    "            coordinates.append((x,y,w,h))\n",
    "            mask[y:y+h, x:x+w] = 255\n",
    "        \n",
    "        # Catching the 0 contours error\n",
    "        if counter == 0:\n",
    "            print('Error! No paintings in this image!')\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "            plt.imshow(mask, cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "        return mask, coordinates\n",
    "    \n",
    "    # Obtain the painting image removing the text. \n",
    "    # It returns the mask where 1 means painting image and 0 text.\n",
    "    def get_mask_text(self, img):\n",
    "        \n",
    "        # Convert the image to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply morphological opening and closing to enhance text-like features using a 9x9 kernel\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9,9))\n",
    "        opening = cv2.morphologyEx(gray, cv2.MORPH_OPEN, kernel)\n",
    "        closing = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        #thresholding the difference to get (hopefully) only the text\n",
    "        x = closing-opening\n",
    "        x = (x>125).astype(np.uint8) \n",
    "\n",
    "        # Dilation to further enhance the text features using a 13x13 kernel\n",
    "        kernel2 = cv2.getStructuringElement(cv2.MORPH_RECT, (13,13))\n",
    "        dilated = cv2.dilate(x, kernel2, iterations=2)\n",
    "\n",
    "        # Find contours \n",
    "        ctns = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Loop through the contours and find rectangular bounding boxes that likely represent text areas\n",
    "        areas = []\n",
    "        for c in ctns[0]:\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            # Filter out rectangles based on certain geometric criteria\n",
    "            if w > h and w/h < 12 and (w*h)/(img.shape[0]*img.shape[1]) < 0.35:\n",
    "                # Shoelace formula for convex shapes\n",
    "                areas.append((cv2.contourArea(c), (x,y,w,h)))\n",
    "        areas = sorted(areas, key=lambda x: x[0], reverse=True)\n",
    "        x, y, w, h = areas[0][1]\n",
    "\n",
    "\n",
    "        # Merge shapes close to the main detected text region (e.g., text broken into separate regions)\n",
    "        for _, shape in areas:\n",
    "            if y > shape[1]-10 and y < shape[1]+10:\n",
    "                if shape[0] < x:\n",
    "                    w = (x+w) - shape[0]\n",
    "                    x = shape[0]\n",
    "                else:\n",
    "                    w = (shape[0]+shape[2]) - x\n",
    "\n",
    "        # Extract the detected text region and apply OCR using Tesseract\n",
    "        binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "        text = pytesseract.image_to_string(binary[y:y+h, x:x+w])\n",
    "        # Clean up the extracted text\n",
    "        text = re.sub(r'[0-9\\n¥“«!|]', '', text)\n",
    "\n",
    "        # Compare the extracted text to known names using the Levenshtein distance to find the closest match from the bag of names\n",
    "        min_dist = 1000000\n",
    "        for name in name_bag:\n",
    "            dist = levenshtein_distance(text, name)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                min_word = name\n",
    "        \n",
    "        # Return the bounding box of the detected text region and the closest matching name\n",
    "        return [x, y, x+w, y+h, min_word]\n",
    "\n",
    "    # Divide the image into blocks\n",
    "    def create_blocks_array(self, image, blockNumber):\n",
    "    \n",
    "        # Set number of slices per axis\n",
    "        axisSlice = int(math.sqrt(blockNumber))\n",
    "\n",
    "        blocksArray = []\n",
    "        # Split the image into vertical blocks\n",
    "        split_h = np.array_split(image, axisSlice, axis = 0)\n",
    "        \n",
    "        for i in range(axisSlice):\n",
    "            for j in range(axisSlice):\n",
    "                # Split vertical blocks into square blocks\n",
    "                split_hv = np.array_split(split_h[i], axisSlice, axis = 1)\n",
    "                blocksArray.append(split_hv[j])\n",
    "        return blocksArray\n",
    "\n",
    "    # Compute the histogram of the image\n",
    "    def create_histogram(self, block, mask, d_hist, mode, bins):\n",
    "        channels = cv2.split(block)\n",
    "\n",
    "        if mode == 'lab' and d_hist < 3: channels = channels[1:]\n",
    "        \n",
    "        range_a, range_b = 256, 256\n",
    "        if mode == 'hsv': range_a = 180\n",
    "\n",
    "        if d_hist == 1:\n",
    "            if mask is None:\n",
    "                # Compute 1D histograms for each channel separately\n",
    "                hist = [cv2.calcHist([chan], [0], None, [bins], [0, range_a if i == 0 else range_b]) for i,chan in enumerate(channels)]\n",
    "            else:\n",
    "                # Compute 1D histograms for each channel separately\n",
    "                hist = [cv2.calcHist([chan[mask!=0]], [0], None, [bins], [0, range_a if i == 0 else range_b]) for i,chan in enumerate(channels)]\n",
    "\n",
    "        elif d_hist == 2:\n",
    "            if mask is None:\n",
    "                # Compute 2D joint histograms for each pair of channels\n",
    "                hist = [cv2.calcHist([channels[i], channels[j]], [0, 1], None, [bins, bins], [0, range_a if i == 0 else range_b, 0, range_b])\n",
    "                            for i in range(len(channels)) for j in range(i+1, len(channels))]\n",
    "            else:\n",
    "                # Compute 2D joint histograms for each pair of channels\n",
    "                hist = [cv2.calcHist([channels[i][mask!=0], channels[j][mask!=0]], [0, 1], None, [bins, bins], [0, range_a if i == 0 else range_b, 0, range_b])\n",
    "                            for i in range(len(channels)) for j in range(i+1, len(channels))]\n",
    "\n",
    "        else:\n",
    "            if mask is None:\n",
    "                # Compute 3D joint histogram for all three channels\n",
    "                hist, _ = np.histogramdd([c.flatten() for c in channels], bins=(bins, bins, bins), range=[(0, range_a), (0, range_b), (0, range_b)])\n",
    "            else:\n",
    "                # Compute 3D joint histogram for all three channels\n",
    "                hist, _ = np.histogramdd([c[mask != 0] for c in channels], bins=(bins, bins, bins), range=[(0, range_a), (0, range_b), (0, range_b)])\n",
    "\n",
    "        return hist\n",
    "    \n",
    "    # Compute the color histogram of the image by blocks\n",
    "    def get_color_features_by_blocks(self, image, level, mode, d_hist, bins, mask_text):\n",
    "\n",
    "        # Get blocks using multi-level resolution\n",
    "        blocksArray = []\n",
    "        for lvl in range(level+1):\n",
    "            for b in self.create_blocks_array(image, (2**lvl)*(2**lvl)):\n",
    "                blocksArray.append(b)\n",
    "\n",
    "        if mask_text is not None:\n",
    "            blocksMasks = []\n",
    "\n",
    "            # We create a mask image blocking the bbox of the text\n",
    "            # That image will be used to compute the histogram of the image without the text\n",
    "            mask_text_image = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "            assert len(mask_text_image.shape) == 2, 'Mask must be a grayscale image'\n",
    "\n",
    "            mask_text_image[mask_text[1]:mask_text[3], mask_text[0]:mask_text[2]] = 0\n",
    "\n",
    "            # It is necessary to create the blocks of the mask image too\n",
    "            for lvl in range(level+1):\n",
    "                for b in self.create_blocks_array(mask_text_image, (2**lvl)*(2**lvl)):\n",
    "                    blocksMasks.append(b)\n",
    "        else:\n",
    "            blocksMasks = [None]*len(blocksArray)\n",
    "\n",
    "        histograms = []\n",
    "        for block, mask_text_block in zip(blocksArray, blocksMasks):\n",
    "            # Compute the histogram of the channel and append it to the list\n",
    "            hist = self.create_histogram(block, mask_text_block, d_hist, mode, bins)\n",
    "            if isinstance(hist, list):\n",
    "                for h in hist:\n",
    "                    histograms.append(h.flatten() / (block.shape[0]*block.shape[1]))\n",
    "            else:\n",
    "                histograms.append(hist.flatten()  / (block.shape[0]*block.shape[1]))\n",
    "            \n",
    "        # Concatenate all histograms into a single feature vector\n",
    "        return np.concatenate(histograms)\n",
    "\n",
    "    def zigzag_scan(self, image):\n",
    "        rows, cols = image.shape\n",
    "        solution = [[] for _ in range(rows + cols - 1)]\n",
    "        \n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                sum_idx = i + j\n",
    "                if (sum_idx % 2 == 0):\n",
    "                    # add at beginning if even index\n",
    "                    solution[sum_idx].insert(0, image[i,j])\n",
    "                else:\n",
    "                    # add at the end if odd index\n",
    "                    solution[sum_idx].append(image[i,j])\n",
    "\n",
    "        # flatten the result\n",
    "        result = np.array([num for sublist in solution for num in sublist])\n",
    "        return result\n",
    "\n",
    "    # Compute different texture features by blocks\n",
    "    def get_texture_features_by_blocks(self, image, level, mode, bins, lbp_radius, dct_n_coeffs, mask_text):\n",
    "        \n",
    "        # Get blocks using multi-level resolution\n",
    "        blocksArray = []\n",
    "        for lvl in range(level+1):\n",
    "            for b in self.create_blocks_array(image, (2**lvl)*(2**lvl)):\n",
    "                blocksArray.append(b)\n",
    "\n",
    "        if mask_text is not None:\n",
    "            blocksMasks = []\n",
    "\n",
    "            # We create a mask image blocking the bbox of the text\n",
    "            # That image will be used to compute the histogram of the image without the text\n",
    "            mask_text_image = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "            # Assign zero to the region corresponding to the text\n",
    "            mask_text_image[mask_text[1]:mask_text[3], mask_text[0]:mask_text[2]] = 0\n",
    "\n",
    "            # It is necessary to create the blocks of the mask image too\n",
    "            for lvl in range(level+1):\n",
    "                for b in self.create_blocks_array(mask_text_image, (2**lvl)*(2**lvl)):\n",
    "                    blocksMasks.append(b)\n",
    "        else:\n",
    "            blocksMasks = [None]*len(blocksArray)\n",
    "\n",
    "        histograms = []\n",
    "        # For each block and its corresponding text mask block, compute texture features\n",
    "        for block, mask_text_block in zip(blocksArray, blocksMasks):\n",
    "            \n",
    "            # Use mask to remove text from block\n",
    "            if mode == \"dct\":\n",
    "                # Use mask to remove text from block\n",
    "                if mask_text_block is not None:\n",
    "                    block[mask_text_block==0] = 0\n",
    "                dct2d_features = dctn(block, axes=(0, 1))\n",
    "\n",
    "                # Get the top 'dct_n_coeffs' coefficients using zigzag scan\n",
    "                zigzag = self.zigzag_scan(dct2d_features)\n",
    "                histograms.append(zigzag[:dct_n_coeffs])\n",
    "\n",
    "            # Compute LBP texture features\n",
    "            elif mode == \"lbp\":\n",
    "                n_points = 8 * lbp_radius\n",
    "                lbp_features = local_binary_pattern(block, P=n_points, R=lbp_radius)\n",
    "\n",
    "                # Compute histogram with or without mask\n",
    "                hist = np.histogram(lbp_features if mask_text_block is None else lbp_features[mask_text_block!=0], bins=bins, range=(0, 256))[0]\n",
    "\n",
    "                # Normalize histogram\n",
    "                histograms.append(hist.flatten() / (lbp_features.shape[0]*lbp_features.shape[1]))\n",
    "            \n",
    "            # Compute Wavelet texture features\n",
    "            elif mode == \"wavelet\":\n",
    "                details = pywt.dwt2(block, 'bior1.3')\n",
    "                approx, (h, v, d) = details # approx captures bigger details (more smooth than the original img), (h, v, d) capture de horizontal, vertical and diagonal \"smaller\" details\n",
    "                \n",
    "                if mask_text_block is not None:\n",
    "                    # Resize the text mask to the size of the wavelet's resulting images\n",
    "                    new_mask = cv2.resize(mask_text_block, approx.shape[::-1]).astype(bool) \n",
    "\n",
    "                # Create an histogram for each wavelet \"image\" and concatenate all of them\n",
    "                final_hist = []\n",
    "                for wt_img in [approx, h, v, d]:\n",
    "                    hist = np.histogram(wt_img if mask_text_block is None else wt_img[new_mask != 0], bins=bins, range=(0, 256))[0]\n",
    "                    final_hist.append(hist.flatten() / (wt_img.shape[0]*wt_img.shape[1]))\n",
    "                histograms.append(np.concatenate(final_hist))\n",
    "            \n",
    "        # Concatenate all histograms into a single feature vector\n",
    "        return np.concatenate(histograms)\n",
    "\n",
    "    def clean_noise(self, image, k):\n",
    "        return cv2.medianBlur(image, k)\n",
    "    \n",
    "    # Load data, calculate background and text masks (if necessary) and compute features\n",
    "    def load_data(self, level = 3, d_hist = 1, bins = 8, remove_background=False, remove_text=False, lbp_radius=1, dct_n_coeffs=32, features_mode='color_features'):\n",
    "        # Get a list of all image file names in the folder\n",
    "        image_files = sorted(glob.glob(self.folder_path+'/*.jpg'))\n",
    "\n",
    "        # Initialize an empty list to store the processed images and masks\n",
    "        processed_features_rgb = dict()\n",
    "        processed_features_hsv = dict()\n",
    "        processed_features_lab = dict()\n",
    "        processed_features_text = dict()\n",
    "        processed_features_dct = dict()\n",
    "        processed_features_lbp = dict()\n",
    "        processed_features_wavelet = dict()\n",
    "        processed_features_combined = dict()\n",
    "        masks, masks_text = [], []\n",
    "\n",
    "        # Iterate over each image file\n",
    "        for f in tqdm.tqdm(image_files):\n",
    "            \n",
    "            # Get the image id from the file name. Depending on the OS, the path separator is different\n",
    "            try:\n",
    "                img_id = int(f.split('\\\\')[-1].split('.')[0].split('_')[-1])\n",
    "            except:\n",
    "                img_id = int(f.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "\n",
    "            # Load the image\n",
    "            image = cv2.imread(f)\n",
    "            # Convert the image from BGR to lab color space\n",
    "            image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
    "            # Convert the image from BGR to HSV color space\n",
    "            image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            # Clean noise of the image using median filter\n",
    "            image, image_lab, image_hsv = self.clean_noise(image, k=3), self.clean_noise(image_lab, k=3), self.clean_noise(image_hsv, k=3)\n",
    "\n",
    "            features_text = []\n",
    "            features_rgb, features_hsv, features_lab = [], [], []\n",
    "            features_dct, features_lbp, features_wavelet = [], [], []\n",
    "\n",
    "            # Remove background (there can be 2 paintings in the same image)\n",
    "            if remove_background:\n",
    "                mask_image, coordinates = self.get_mask(image)\n",
    "               \n",
    "                # Remove the text from each image\n",
    "                if remove_text:\n",
    "                    coordinates = sorted(coordinates, key=lambda x: (x[0], x[1]))\n",
    "                    masks_text_i = [[None]]*len(coordinates)\n",
    "\n",
    "                    # coordinates contains the coordinates of the paintings mask in the image\n",
    "                    # We iterate over each masked painting and get the text mask for each one\n",
    "                    # We hace to recover the original coordinates for the text mask\n",
    "                    for i, (x,y,w,h) in enumerate(coordinates):\n",
    "                        x_text, y_text, w_text, h_text, text = self.get_mask_text(image[y:y+h, x:x+w])\n",
    "                        features_text.append(text)\n",
    "                        masks_text_i[i] = [x+x_text, y+y_text, x+x_text+w_text, y+y_text+h_text]\n",
    "                    masks_text.append(masks_text_i)\n",
    "                else:\n",
    "                    for i in range(len(coordinates)):\n",
    "                        masks_text.append([None])\n",
    "\n",
    "            else:\n",
    "                mask_image = None\n",
    "\n",
    "                # if there is no background, the mask is the whole image\n",
    "                coordinates = [[0,0,image.shape[1],image.shape[0]]] \n",
    "                if remove_text:\n",
    "                    x_text, y_text, w_text, h_text, text = self.get_mask_text(image)\n",
    "                    features_text.append(text)\n",
    "                    masks_text.append([[x_text, y_text, w_text, h_text]])\n",
    "                else:\n",
    "                    masks_text.append([None])\n",
    "\n",
    "            masks.append(mask_image)\n",
    "        \n",
    "            for i in range(len(coordinates)):\n",
    "                x,y,w,h = coordinates[i]\n",
    "\n",
    "                relative_mask_text = None\n",
    "                if masks_text[-1][i] is not None:\n",
    "                    relative_mask_text = [masks_text[-1][i][0]-x, masks_text[-1][i][1]-y, masks_text[-1][i][2]-x, masks_text[-1][i][3]-y]\n",
    "\n",
    "                if features_mode == 'color_features' or features_mode == 'combined':    \n",
    "                    # Get the features of every masked image\n",
    "                    f = self.get_color_features_by_blocks(image[y:y+h, x:x+w], level-1, 'rgb', d_hist, bins, mask_text=relative_mask_text)\n",
    "                    f_hsv = self.get_color_features_by_blocks(image_hsv[y:y+h, x:x+w], level, 'hsv', d_hist, bins, mask_text=relative_mask_text)\n",
    "                    f_lab = self.get_color_features_by_blocks(image_lab[y:y+h, x:x+w], level, 'lab', d_hist, bins, mask_text=relative_mask_text)\n",
    "\n",
    "                    features_rgb.append(f)\n",
    "                    features_hsv.append(f_hsv)\n",
    "                    features_lab.append(f_lab)\n",
    "\n",
    "                if features_mode == 'texture_features' or features_mode == 'combined':\n",
    "                    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                    \n",
    "                    f_dct = self.get_texture_features_by_blocks(image_gray[y:y+h, x:x+w], level, 'dct', bins, lbp_radius, dct_n_coeffs, mask_text=relative_mask_text)\n",
    "                    f_lbp = self.get_texture_features_by_blocks(image_gray[y:y+h, x:x+w], level, 'lbp', bins, lbp_radius, dct_n_coeffs, mask_text=relative_mask_text)\n",
    "                    f_wavelet = self.get_texture_features_by_blocks(image_gray[y:y+h, x:x+w], level, 'wavelet', bins, lbp_radius, dct_n_coeffs, mask_text=relative_mask_text)\n",
    "                    \n",
    "                    features_dct.append(f_dct)\n",
    "                    features_lbp.append(f_lbp)\n",
    "                    features_wavelet.append(f_wavelet)\n",
    "\n",
    "            # Append the features to the dict\n",
    "            if features_mode == 'texture_features':\n",
    "                processed_features_dct[img_id] = features_dct\n",
    "                processed_features_lbp[img_id] = features_lbp\n",
    "                processed_features_wavelet[img_id] = features_wavelet\n",
    "            \n",
    "            elif features_mode == 'text_features':\n",
    "                processed_features_text[img_id] = features_text\n",
    "\n",
    "            elif features_mode == 'color_features':\n",
    "                processed_features_rgb[img_id] = features_rgb\n",
    "                processed_features_hsv[img_id] = features_hsv\n",
    "                processed_features_lab[img_id] = features_lab\n",
    "            \n",
    "            elif features_mode == 'combined':\n",
    "                if len(features_rgb) > 1:\n",
    "                    assert len(features_rgb) == len(features_lbp) == len(features_text), 'The number of features must be the same for each mode!'\n",
    "                    processed_features_combined[img_id] = [[features_rgb[0], features_wavelet[0], features_text[0]], [features_rgb[1], features_wavelet[1], features_text[1]]]\n",
    "                else:\n",
    "                    processed_features_combined[img_id] = [[features_rgb, features_wavelet, features_text]]\n",
    "            \n",
    "        # Change the return depending on the mode\n",
    "        if features_mode == 'texture_features':\n",
    "            return processed_features_dct, processed_features_lbp, processed_features_wavelet, masks, masks_text\n",
    "        elif features_mode == 'text_features':\n",
    "            return processed_features_text, masks, masks_text\n",
    "        elif features_mode == 'color_features':\n",
    "            return processed_features_rgb, processed_features_hsv, processed_features_lab, masks, masks_text\n",
    "        elif features_mode == 'combined':\n",
    "            return processed_features_combined, masks, masks_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://github.com/benhamner/Metrics -> Metrics.Python.ml_metrics.average_precision.py\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# Copied from https://github.com/benhamner/Metrics -> Metrics.Python.ml_metrics.average_precision.py\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for a,p in zip(actual, predicted):\n",
    "        for a_i, p_i in zip(a,p):\n",
    "            result.append(apk([a_i],p_i,k))\n",
    "    return np.mean(result)\n",
    "\n",
    "# compute the histogram intersection between two feature vectors\n",
    "def histogram_intersection(hist1, hist2, normalized=False):\n",
    "    if normalized:\n",
    "        return np.sum(np.minimum(hist1, hist2)) / np.sum(np.maximum(hist1, hist2))\n",
    "    else:\n",
    "        return np.sum(np.minimum(hist1, hist2))\n",
    "\n",
    "# compute the euclidian distance between two feature vectors\n",
    "def euclidian_distance(hist1, hist2):\n",
    "    return np.sqrt(np.sum(np.square(hist1 - hist2)))\n",
    "\n",
    "# compute the chi-squared distance between two feature vectors\n",
    "def chi_squared_distance(hist1, hist2):\n",
    "    return np.sum(np.square(hist1 - hist2) / (hist1 + hist2 + 1e-10))\n",
    "\n",
    "# compute the bhattacharyya distance between two feature vectors\n",
    "def bhattacharyya_distance(hist1, hist2):\n",
    "    # Ensure that both histograms have the same shape\n",
    "    assert hist1.shape == hist2.shape, \"Histograms must have the same shape\"\n",
    "    # Calculate the Bhattacharyya coefficient\n",
    "    bhattacharyya_coeff = np.sum(np.sqrt(hist1 * hist2))\n",
    "    # Calculate the Bhattacharyya distance\n",
    "    bhattacharyya_distance = -np.log(bhattacharyya_coeff)\n",
    "    return bhattacharyya_distance\n",
    "\n",
    "# compute the Helling distance (Hellinger kernel) between two feature vectors\n",
    "def hellinger_kernel(hist1, hist2):\n",
    "    return np.sum(np.sqrt(hist1*hist2))\n",
    "\n",
    "def custom_leveshtein_distance(s1, s2, normalized=False):\n",
    "    if normalized:\n",
    "        return (max(len(s1), len(s2)) - levenshtein_distance(s1, s2)) / max(len(s1), len(s2))\n",
    "    else:\n",
    "        return levenshtein_distance(s1, s2)\n",
    "\n",
    "# based on the solution in https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation by @Martin Thoma\n",
    "def get_iou(bb1, bb2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bb1 : list ['x1', 'x2', 'y1', 'y2']\n",
    "        The (x1, y1) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "    bb2 : list ['x1', 'x2', 'y1', 'y2']\n",
    "        The (x1, y1) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        in [0, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1[0], bb2[0])\n",
    "    y_top = max(bb1[1], bb2[1])\n",
    "    x_right = min(bb1[2], bb2[2])\n",
    "    y_bottom = min(bb1[3], bb2[3])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou\n",
    "\n",
    "# Calculate Intersection over Union (IoU) for each query image and plot the results (if necessary).\n",
    "def calculate_iou(gt_bboxes, pred_bboxes, do_plot=False):\n",
    "    plot_rows, plot_cols = 4, 4\n",
    "\n",
    "    # Plot config\n",
    "    if do_plot:\n",
    "        fig, ax = plt.subplots(nrows=plot_rows, ncols=plot_cols, figsize=(20, 20))\n",
    "\n",
    "    # Get mean IoU score\n",
    "    results = []\n",
    "    for i, (q_gt, q_pred) in enumerate(zip(gt_bboxes, pred_bboxes)):\n",
    "        for bbox_gt, bbox_pred in zip(q_gt, q_pred):\n",
    "            bbox_gt_new = [bbox_gt[0][0], bbox_gt[0][1], bbox_gt[2][0], bbox_gt[2][1]]\n",
    "            results.append(get_iou(bbox_pred, bbox_gt_new))\n",
    "\n",
    "        if i < plot_rows*plot_cols and do_plot:\n",
    "            im = cv2.imread(f\"data/qsd1_w2/{str(i).zfill(5)}.jpg\")\n",
    "            plt_idx = np.unravel_index(i, (plot_rows, plot_cols))\n",
    "            ax[plt_idx].imshow(im)\n",
    "            ax[plt_idx].add_patch(plt.Rectangle(\n",
    "                (bbox_gt_new[0], bbox_gt_new[1]), \n",
    "                bbox_gt_new[2]- bbox_gt_new[0],\n",
    "                bbox_gt_new[3]- bbox_gt_new[1],\n",
    "                edgecolor=\"green\", facecolor=\"none\", lw=2))\n",
    "            ax[plt_idx].add_patch(plt.Rectangle(\n",
    "                (bbox_pred[0], bbox_pred[1]), \n",
    "                bbox_pred[2]- bbox_pred[0],\n",
    "                bbox_pred[3]- bbox_pred[1],\n",
    "                edgecolor=\"red\", facecolor=\"none\", lw=2))\n",
    "    if do_plot: plt.show()\n",
    "\n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the closest k DDBB image for query images determined by the similarity function. \n",
    "# The features have been previously calculated from the developed method.\n",
    "# It returns a list of lists with the k closest images for each query image. \n",
    "def compare_images(query_features, bbdd_features, k, sim_func, param=None, filter=False, combine=False):\n",
    "    \n",
    "    result = []\n",
    "    for id1,f1 in query_features.items():\n",
    "        result_i = []\n",
    "        for f_i in f1:\n",
    "            distances = []\n",
    "            for id2,f2 in bbdd_features.items():\n",
    "                text_bd = get_text_bbdd(f'data/BBDD/bbdd_{str(id2).zfill(5)}.txt')\n",
    "\n",
    "                # Use the provided similarity function for comparing both paintings\n",
    "                if not filter and not combine:\n",
    "                    distances.append((id2, sim_func(f_i,f2)))\n",
    "                    continue\n",
    "                \n",
    "                for f2_i in f2:\n",
    "                    # First filter those paintings that have the same author. Then, compute similarity/distance for retrieval\n",
    "                    if filter and text_bd == f_i[-1][0]:\n",
    "                       distances.append((id2, param*sim_func(f_i[0][0], f2_i[0][0], normalized=True) + (1-param)*sim_func(f_i[1][0], f2_i[1][0], normalized=True)))\n",
    "                    \n",
    "                    # Use weighted sum between the color, texture and text scores as the similarity score for the retrieval\n",
    "                    elif combine:\n",
    "                        # If the similarity function is a distance \n",
    "                        if sim_func in [euclidian_distance, chi_squared_distance, bhattacharyya_distance, levenshtein_distance]:\n",
    "                            distances.append(\n",
    "                                (id2, \n",
    "                                param[0]*sim_func(f_i[0][0], f2_i[0][0], normalized=True) + \n",
    "                                param[1]*sim_func(f_i[1][0], f2_i[1][0], normalized=True) +\n",
    "                                param[2]*(1 - custom_leveshtein_distance(f_i[2][0], text_bd, normalized=True))\n",
    "                                ))\n",
    "                        else:\n",
    "                            distances.append(\n",
    "                                (id2, \n",
    "                                param[0]*sim_func(f_i[0][0], f2_i[0][0], normalized=True) + \n",
    "                                param[1]*sim_func(f_i[1][0], f2_i[1][0], normalized=True) +\n",
    "                                param[2]*custom_leveshtein_distance(f_i[2][0], text_bd, normalized=True)\n",
    "                                ))\n",
    "                \n",
    "            #get k smallest values from distances   \n",
    "            if sim_func in [euclidian_distance, chi_squared_distance, bhattacharyya_distance, levenshtein_distance]:\n",
    "                k_smallest = sorted(distances, reverse=False, key=lambda x: x[1])[:k]\n",
    "            else:\n",
    "                k_smallest = sorted(distances, reverse=True, key=lambda x: x[1])[:k]\n",
    "            result_i.append((id1, k_smallest))\n",
    "            \n",
    "        result.append(result_i)\n",
    "    \n",
    "    # Transform the result into the required format\n",
    "    result2 = []\n",
    "    for x in result:\n",
    "        result2_i = []\n",
    "        for y in x:\n",
    "            result2_i.append([z[0] for z in y[1]])\n",
    "        result2.append(result2_i)\n",
    "    \n",
    "    return result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader objects for both the database and the queries\n",
    "data_loader = DataLoader('data/BBDD')\n",
    "data_loader_qsd1_w3 = DataLoader('data/qsd1_w3')\n",
    "data_loader_qsd2_w3 = DataLoader('data/qsd2_w3')\n",
    "\n",
    "# Load ground truth files for each query\n",
    "with open('data/qsd1_w3/gt_corresps.pkl', 'rb') as f:\n",
    "    gt_w3_1 = pickle.load(f)\n",
    "\n",
    "with open('data/qsd2_w3/gt_corresps.pkl', 'rb') as f:\n",
    "    gt_w3_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation results\n",
    "\n",
    "### Task 2: \n",
    "\n",
    "Evaluate query system using QSD1-W3 using only text and a similarity metric to compare text.\n",
    "\n",
    "Compare with retrieval using only color descriptors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define best hyperparameters configuration\n",
    "features_mode = 'text_features'\n",
    "k = 5\n",
    "\n",
    "# Calculate and store the bbdd text features\n",
    "bbdd_text = dict()\n",
    "for i, text_file in enumerate(glob.glob(f'data/BBDD/*.txt')):\n",
    "    # read text file\n",
    "    with open(text_file, 'r') as f:\n",
    "        line = f.readlines()\n",
    "    \n",
    "    for l in line:\n",
    "        if re.search(r\"\\('([^']+)'\", l.split(',')[0]):\n",
    "            author = re.search(r\"\\('([^']+)'\", l.split(',')[0]).group(1)\n",
    "            bbdd_text[i] = author\n",
    "\n",
    "query_text, _, _ = data_loader_qsd1_w3.load_data(remove_text=True, features_mode=features_mode)\n",
    "\n",
    "results = compare_images(query_text, bbdd_text, k, levenshtein_distance)\n",
    "mapk_1 = mapk(gt_w3_1, results, k)\n",
    "print(f'MAP@{k} for qsd1_w3: {mapk_1}')    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "features_mode = 'color_features'\n",
    "# Compute features for the database and the query images\n",
    "features_rgb, features_hsv, features_lab, _, _ = data_loader.load_data(features_mode=features_mode, remove_background=False, level=2, d_hist=2, bins=8)\n",
    "features_rgb_q1_w2, features_hsv_q1_w2, features_lab_q1_w2, _, _ = data_loader_qsd1_w3.load_data(features_mode=features_mode, remove_background=False, remove_text=True, level=2, d_hist=2, bins=8)\n",
    "\n",
    "# Query 1: Results and mAP@k\n",
    "for sim_func in [chi_squared_distance, histogram_intersection]:\n",
    "\n",
    "    results_rgb_q1_w3 = compare_images(features_rgb_q1_w2, features_rgb, k, sim_func)\n",
    "    results_hsv_q1_w3 = compare_images(features_hsv_q1_w2, features_hsv, k, sim_func)\n",
    "    results_lab_q1_w3 = compare_images(features_lab_q1_w2, features_lab, k, sim_func)\n",
    "\n",
    "    mapk_rgb_1 = mapk(gt_w3_1, results_rgb_q1_w3, k)\n",
    "    mapk_hsv_1 = mapk(gt_w3_1, results_hsv_q1_w3, k)\n",
    "    mapk_lab_1 = mapk(gt_w3_1, results_lab_q1_w3, k)\n",
    "\n",
    "    print(f'RGB, {sim_func.__name__} = \\tmAP@1: {mapk_rgb_1}')\n",
    "    print(f'HSV, {sim_func.__name__} = \\tmAP@1: {mapk_hsv_1}')\n",
    "    print(f'LAB, {sim_func.__name__} = \\tmAP@1: {mapk_lab_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:\n",
    "\n",
    "Implement texture descriptors (LBP, DCT, wavelet-based, etc.)\n",
    "\n",
    "Evaluate query system using QSD1-W3 using only texture descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid search for hyperparameters configuration\n",
    "k = [1,5]\n",
    "bins = 8\n",
    "dct_n_coeffs = [64,128,512]\n",
    "features_mode = \"texture_features\"\n",
    "level = [1,2,3]\n",
    "lbp_radius = [1,2,3]\n",
    "\n",
    "# Compute features for the database and the query images\n",
    "for dct_n_coeff, lbp_r in zip(dct_n_coeffs, lbp_radius):\n",
    "    for l in level:\n",
    "\n",
    "        features_dct, features_lbp, features_wavelet, _, _ = data_loader.load_data(\n",
    "            remove_background=False, remove_text=False, level=l, \n",
    "            bins=bins, dct_n_coeffs=dct_n_coeff, \n",
    "            features_mode=features_mode, lbp_radius=lbp_r)\n",
    "        \n",
    "        features_dct_q1_w3, features_lbp_q1_w3, features_wavelet_q1_w3, _, _ = data_loader_qsd1_w3.load_data(\n",
    "            remove_background=False, remove_text=True, level=l, \n",
    "            bins=bins, dct_n_coeffs=dct_n_coeff, \n",
    "            features_mode=features_mode, lbp_radius=lbp_r)\n",
    "\n",
    "        # Iterating through similarity functions and k values for mAP calculation\n",
    "        for sim_func in [chi_squared_distance, histogram_intersection]:\n",
    "            for ki in k:\n",
    "\n",
    "                # Compare images to get similar results for different feature sets\n",
    "                results_dct_q1_w3 = compare_images(features_dct_q1_w3, features_dct, ki, sim_func)\n",
    "                results_lbp_q1_w3 = compare_images(features_lbp_q1_w3, features_lbp, ki, sim_func)\n",
    "                results_wavelet_q1_w3 = compare_images(features_wavelet_q1_w3, features_wavelet, ki, sim_func)\n",
    "\n",
    "                # Calculate mAP values for the retrieved results\n",
    "                mapk_dct = mapk(gt_w3_1, results_dct_q1_w3, ki)\n",
    "                mapk_lbp = mapk(gt_w3_1, results_lbp_q1_w3, ki)\n",
    "                mapk_wavelet = mapk(gt_w3_1, results_wavelet_q1_w3, ki)\n",
    "\n",
    "                # Print the mAP results\n",
    "                print(f'DCT, {sim_func.__name__} = \\tmAP@{ki}: {mapk_dct}')\n",
    "                print(f'LBP, {sim_func.__name__} = \\tmAP@{ki}: {mapk_lbp}')\n",
    "                print(f'Wavelet, {sim_func.__name__} = \\tmAP@{ki}: {mapk_wavelet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4:\n",
    "\n",
    "Combine descriptors. Evaluate query system using QSD2-W3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [1,5]\n",
    "features_mode = 'combined'\n",
    "d_hist = 2\n",
    "level = 3\n",
    "bins = 8\n",
    "# Compute features for the database and the query images\n",
    "features, _, _ = data_loader.load_data(features_mode=features_mode, remove_background=False, level=level, d_hist=d_hist, bins=bins, lbp_radius=None)\n",
    "features_q2_w3, _, _ = data_loader_qsd2_w3.load_data(features_mode=features_mode, remove_background=False, remove_text=True, level=level, d_hist=d_hist, bins=bins, lbp_radius=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for lambda weight values\n",
    "params = [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "for p in params:\n",
    "    for ki in k:\n",
    "        results_q2_w3 = compare_images(features_q2_w3, features, ki, histogram_intersection, filter=True, param=p)\n",
    "        mapk_1 = mapk(gt_w3_2, results_q2_w3, ki)\n",
    "        print(f'Combined param {p}, histogram_intersection = \\tmAP@{ki}: {mapk_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best result\n",
    "results_q2_w3 = compare_images(features_q2_w3, features, 5, histogram_intersection, filter=True, param=0.5)\n",
    "mapk_1 = mapk(gt_w3_2, results_q2_w3, 1)\n",
    "mapk_5 = mapk(gt_w3_2, results_q2_w3, 5)\n",
    "print(f'Combined param {0.5}, histogram_intersection = \\tmAP@{1}: {mapk_1} mAP@{5}: {mapk_5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all three modalities (color, texture and text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for lambda weight values\n",
    "params_list = [\n",
    "    [1, 1, 1], [1, 1, 0], [1, 0, 1], [1, 1, 0],\n",
    "    [0.5, 0.25, 0.25], [0.25, 0.5, 0.25], [0.25, 0.25, 0.5],\n",
    "    [0.5, 0.5, 0.25], [0.25, 0.5, 0.5], [0.5, 0.25, 0.5],\n",
    "    [0.7, 0.15, 0.15], [0.15, 0.7, 0.15], [0.15, 0.15, 0.7],\n",
    "]\n",
    "\n",
    "results = []\n",
    "params_list = list(itertools.product([0,1,2,4,6,10,15], repeat=3))\n",
    "for params in tqdm.tqdm(params_list):\n",
    "    # Compute features for the database and the query images\n",
    "    results_q2_w3 = compare_images(features_q2_w3, features, 5, histogram_intersection, combine=True, param=params)\n",
    "    mapk_1 = mapk(gt_w3_2, results_q2_w3, 1)\n",
    "    mapk_5 = mapk(gt_w3_2, results_q2_w3, 5)\n",
    "    results.append({\"params\": params, \"map1\": mapk_1, \"map5\": mapk_5})\n",
    "    print(f'Combined param {params}, histogram_intersection = \\tmAP@{1}: {mapk_1}, mAP@{5}: {mapk_5}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
