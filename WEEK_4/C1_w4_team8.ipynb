{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1 - Content Based Image Retrieval\n",
    "### Team 8 - Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, glob, math, tqdm, pickle, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import pytesseract\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "import pywt\n",
    "from skimage.feature import local_binary_pattern\n",
    "from scipy.fftpack import dctn\n",
    "\n",
    "import utils\n",
    "\n",
    "#autoreload modules when code is run\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Path to the OCR executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Luis\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set to store unique names of authors.\n",
    "name_bag = set()\n",
    "\n",
    "for folder in ['BBDD', 'qsd1_w4']:\n",
    "    # Loop through each .txt file inside the folder.\n",
    "    for text_file in glob.glob(f'data/{folder}/*.txt'):\n",
    "        # Extract the specific text pattern from the file and add it to the set.\n",
    "        name_bag.add(utils.get_text_bbdd(text_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "\n",
    "    # Divide the image into blocks\n",
    "    def create_blocks_array(self, image, blockNumber):\n",
    "        # Set number of slices per axis\n",
    "        axisSlice = int(math.sqrt(blockNumber))\n",
    "\n",
    "        blocksArray = []\n",
    "        # Split the image into vertical blocks\n",
    "        split_h = np.array_split(image, axisSlice, axis = 0)\n",
    "        \n",
    "        for i in range(axisSlice):\n",
    "            for j in range(axisSlice):\n",
    "                # Split vertical blocks into square blocks\n",
    "                split_hv = np.array_split(split_h[i], axisSlice, axis = 1)\n",
    "                blocksArray.append(split_hv[j])\n",
    "        return blocksArray\n",
    "\n",
    "    # Compute the histogram of the image\n",
    "    def create_histogram(self, block, mask, d_hist, bins):\n",
    "        \n",
    "        channels = cv2.split(block)\n",
    "        range_a, range_b = 256, 256\n",
    "\n",
    "        if d_hist == 1:\n",
    "            if mask is None:\n",
    "                # Compute 1D histograms for each channel separately\n",
    "                hist = [cv2.calcHist([chan], [0], None, [bins], [0, range_a if i == 0 else range_b]) for i,chan in enumerate(channels)]\n",
    "            else:\n",
    "                # Compute 1D histograms for each channel separately\n",
    "                hist = [cv2.calcHist([chan[mask!=0]], [0], None, [bins], [0, range_a if i == 0 else range_b]) for i,chan in enumerate(channels)]\n",
    "\n",
    "        elif d_hist == 2:\n",
    "            if mask is None:\n",
    "                # Compute 2D joint histograms for each pair of channels\n",
    "                hist = [cv2.calcHist([channels[i], channels[j]], [0, 1], None, [bins, bins], [0, range_a if i == 0 else range_b, 0, range_b])\n",
    "                            for i in range(len(channels)) for j in range(i+1, len(channels))]\n",
    "            else:\n",
    "                # Compute 2D joint histograms for each pair of channels\n",
    "                hist = [cv2.calcHist([channels[i][mask!=0], channels[j][mask!=0]], [0, 1], None, [bins, bins], [0, range_a if i == 0 else range_b, 0, range_b])\n",
    "                            for i in range(len(channels)) for j in range(i+1, len(channels))]\n",
    "\n",
    "        else:\n",
    "            if mask is None:\n",
    "                # Compute 3D joint histogram for all three channels\n",
    "                hist, _ = np.histogramdd([c.flatten() for c in channels], bins=(bins, bins, bins), range=[(0, range_a), (0, range_b), (0, range_b)])\n",
    "            else:\n",
    "                # Compute 3D joint histogram for all three channels\n",
    "                hist, _ = np.histogramdd([c[mask != 0] for c in channels], bins=(bins, bins, bins), range=[(0, range_a), (0, range_b), (0, range_b)])\n",
    "\n",
    "        return hist\n",
    "    \n",
    "    # Compute the color histogram of the image by blocks\n",
    "    def get_color_features_by_blocks(self, image, level, d_hist, bins, mask_text):\n",
    "\n",
    "        # Get blocks using multi-level resolution\n",
    "        blocksArray = []\n",
    "        for lvl in range(level+1):\n",
    "            for b in self.create_blocks_array(image, (2**lvl)*(2**lvl)):\n",
    "                blocksArray.append(b)\n",
    "\n",
    "        if mask_text is not None:\n",
    "            blocksMasks = []\n",
    "\n",
    "            # We create a mask image blocking the bbox of the text\n",
    "            # That image will be used to compute the histogram of the image without the text\n",
    "            mask_text_image = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "            mask_text_image[mask_text[1]:mask_text[3], mask_text[0]:mask_text[2]] = 0\n",
    "\n",
    "            # It is necessary to create the blocks of the mask image too\n",
    "            for lvl in range(level+1):\n",
    "                for b in self.create_blocks_array(mask_text_image, (2**lvl)*(2**lvl)):\n",
    "                    blocksMasks.append(b)\n",
    "        else:\n",
    "            blocksMasks = [None]*len(blocksArray)\n",
    "\n",
    "        histograms = []\n",
    "        for block, mask_text_block in zip(blocksArray, blocksMasks):\n",
    "            # Compute the histogram of the channel and append it to the list\n",
    "            hist = self.create_histogram(block, mask_text_block, d_hist, bins)\n",
    "            if isinstance(hist, list):\n",
    "                for h in hist:\n",
    "                    histograms.append(h.flatten() / (block.shape[0]*block.shape[1]))\n",
    "            else:\n",
    "                histograms.append(hist.flatten()  / (block.shape[0]*block.shape[1]))\n",
    "            \n",
    "        # Concatenate all histograms into a single feature vector\n",
    "        return np.concatenate(histograms)\n",
    "\n",
    "    def zigzag_scan(self, image):\n",
    "        rows, cols = image.shape\n",
    "        solution = [[] for _ in range(rows + cols - 1)]\n",
    "        \n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                sum_idx = i + j\n",
    "                if (sum_idx % 2 == 0):\n",
    "                    # add at beginning if even index\n",
    "                    solution[sum_idx].insert(0, image[i,j])\n",
    "                else:\n",
    "                    # add at the end if odd index\n",
    "                    solution[sum_idx].append(image[i,j])\n",
    "\n",
    "        # flatten the result\n",
    "        result = np.array([num for sublist in solution for num in sublist])\n",
    "        return result\n",
    "\n",
    "    # Compute different texture features by blocks\n",
    "    def get_texture_features_by_blocks(self, image, level, bins, mask_text):\n",
    "        \n",
    "        # Get blocks using multi-level resolution\n",
    "        blocksArray = []\n",
    "        for lvl in range(level+1):\n",
    "            for b in self.create_blocks_array(image, (2**lvl)*(2**lvl)):\n",
    "                blocksArray.append(b)\n",
    "\n",
    "        if mask_text is not None:\n",
    "            blocksMasks = []\n",
    "\n",
    "            # We create a mask image blocking the bbox of the text\n",
    "            # That image will be used to compute the histogram of the image without the text\n",
    "            mask_text_image = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "            # Assign zero to the region corresponding to the text\n",
    "            mask_text_image[mask_text[1]:mask_text[3], mask_text[0]:mask_text[2]] = 0\n",
    "\n",
    "            # It is necessary to create the blocks of the mask image too\n",
    "            for lvl in range(level+1):\n",
    "                for b in self.create_blocks_array(mask_text_image, (2**lvl)*(2**lvl)):\n",
    "                    blocksMasks.append(b)\n",
    "        else:\n",
    "            blocksMasks = [None]*len(blocksArray)\n",
    "\n",
    "        histograms = []\n",
    "        # For each block and its corresponding text mask block, compute texture features\n",
    "        for block, mask_text_block in zip(blocksArray, blocksMasks):\n",
    "            \n",
    "            details = pywt.dwt2(block, 'bior1.3')\n",
    "            approx, (h, v, d) = details # approx captures bigger details (more smooth than the original img), (h, v, d) capture de horizontal, vertical and diagonal \"smaller\" details\n",
    "            \n",
    "            if mask_text_block is not None:\n",
    "                # Resize the text mask to the size of the wavelet's resulting images\n",
    "                new_mask = cv2.resize(mask_text_block, approx.shape[::-1]).astype(bool) \n",
    "\n",
    "            # Create an histogram for each wavelet \"image\" and concatenate all of them\n",
    "            final_hist = []\n",
    "            for wt_img in [approx, h, v, d]:\n",
    "                hist = np.histogram(wt_img if mask_text_block is None else wt_img[new_mask != 0], bins=bins, range=(0, 256))[0]\n",
    "                final_hist.append(hist.flatten() / (wt_img.shape[0]*wt_img.shape[1]))\n",
    "            histograms.append(np.concatenate(final_hist))\n",
    "            \n",
    "        # Concatenate all histograms into a single feature vector\n",
    "        return np.concatenate(histograms)\n",
    "\n",
    "    def get_features_by_keypoints(self, gray, mode, n_features, mask):\n",
    "        \n",
    "        if mode == 'sift':\n",
    "            # SIFT Detector\n",
    "            sift = cv2.SIFT_create(nfeatures=n_features)\n",
    "            _, des = sift.detectAndCompute(gray, mask)\n",
    "\n",
    "        elif mode == 'orb':\n",
    "            # ORB Detector\n",
    "            orb = cv2.ORB_create(nfeatures=n_features)\n",
    "            _, des = orb.detectAndCompute(gray, mask)\n",
    "\n",
    "        elif mode == 'akaze':\n",
    "            thres = 0.005\n",
    "            # AKAZE Detector\n",
    "            akaze = cv2.AKAZE_create(threshold=thres)\n",
    "            _, des = akaze.detectAndCompute(gray, mask)\n",
    "\n",
    "            while des is None or des.shape[0] < n_features:\n",
    "                if str(thres)[-1] == '1': \n",
    "                    thres = thres / 2\n",
    "                else:\n",
    "                    thres /= 5\n",
    "                \n",
    "                akaze = cv2.AKAZE_create(threshold=thres)\n",
    "                _, des = akaze.detectAndCompute(gray, mask)\n",
    "\n",
    "                if thres < 1e-6:\n",
    "                    break\n",
    "\n",
    "        return des\n",
    "\n",
    "    def clean_noise(self, image, k):\n",
    "        return cv2.medianBlur(image, k)\n",
    "    \n",
    "    # Load data, calculate background and text masks (if necessary) and compute features\n",
    "    def load_data(self, level = 3, d_hist = 1, bins = 8, n_features=2048, keypoint_mode='sift', remove_background=False, remove_text=False, features_mode='color_features'):\n",
    "        # Get a list of all image file names in the folder\n",
    "        image_files = sorted(glob.glob(self.folder_path+'/*.jpg'))\n",
    "\n",
    "        # Initialize an empty list to store the processed images and masks\n",
    "        processed_features = dict()\n",
    "        masks, masks_text = [], []\n",
    "\n",
    "        # Iterate over each image file\n",
    "        for f in tqdm.tqdm(image_files):\n",
    "            \n",
    "            # Get the image id from the file name. Depending on the OS, the path separator is different\n",
    "            try:\n",
    "                img_id = int(f.split('\\\\')[-1].split('.')[0].split('_')[-1])\n",
    "            except:\n",
    "                img_id = int(f.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "\n",
    "            # Load the image in BGR format\n",
    "            image = cv2.imread(f)\n",
    "\n",
    "            # Clean noise of the image using median filter\n",
    "            image = self.clean_noise(image, k=3)\n",
    "\n",
    "            # Convert the image to grayscale\n",
    "            image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            features_rgb, features_wavelet, features_text, features_keypoints = [], [], [], []\n",
    "\n",
    "            # Remove background (there can be 2 paintings in the same image)\n",
    "            if remove_background:\n",
    "                mask_image, coordinates = utils.get_mask(image_gray)\n",
    "                n_paintings = len(coordinates)\n",
    "               \n",
    "                # Remove the text from each image\n",
    "                if remove_text:\n",
    "                    coordinates = sorted(coordinates, key=lambda x: (x[0], x[1]))\n",
    "                    masks_text_i = [[None]]*n_paintings\n",
    "                    # coordinates contains the coordinates of the paintings mask in the image\n",
    "                    # We iterate over each masked painting and get the text mask for each one\n",
    "                    # We hace to recover the original coordinates for the text mask\n",
    "                    for i, (x,y,w,h) in enumerate(coordinates):\n",
    "                        x_text, y_text, x_text_max, y_text_max, text = utils.get_mask_text(image_gray[y:y+h, x:x+w], name_bag=name_bag)\n",
    "                        features_text.append(text)\n",
    "                        masks_text_i[i] = [x+x_text, y+y_text, x+x_text_max, y+y_text_max]\n",
    "                    masks_text.append(masks_text_i)\n",
    "                else:\n",
    "                    masks_text.extend([[None] for _ in range(n_paintings)])\n",
    "\n",
    "            else:\n",
    "                mask_image = None\n",
    "\n",
    "                # if there is no background, the mask is the whole image\n",
    "                coordinates = [[0,0,image.shape[1],image.shape[0]]]\n",
    "                n_paintings = 1\n",
    "                if remove_text:\n",
    "                    x_text, y_text, x_text_max, y_text_max, text = utils.get_mask_text(image_gray, name_bag=name_bag)\n",
    "                    features_text.append(text)\n",
    "                    masks_text.append([[x_text, y_text, x_text_max, y_text_max]])\n",
    "                else:\n",
    "                    masks_text.append([None])\n",
    "\n",
    "            masks.append(mask_image)\n",
    "        \n",
    "            for i in range(n_paintings):\n",
    "                x,y,w,h = coordinates[i]\n",
    "\n",
    "                mask_painting = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "                mask_painting[y:y+h, x:x+w] = 255\n",
    "\n",
    "                relative_mask_text = None\n",
    "                if masks_text[-1][i] is not None:\n",
    "                    relative_mask_text = [masks_text[-1][i][0]-x, masks_text[-1][i][1]-y, masks_text[-1][i][2]-x, masks_text[-1][i][3]-y]\n",
    "                    # Remove the text from the image \n",
    "                    mask_painting[masks_text[-1][i][1]:masks_text[-1][i][3], masks_text[-1][i][0]:masks_text[-1][i][2]] = 0\n",
    "\n",
    "                if features_mode == 'color_features' or features_mode == 'combined':    \n",
    "                    # Get the features of every masked image\n",
    "                    f = self.get_color_features_by_blocks(image[y:y+h, x:x+w], level, d_hist, bins, mask_text=relative_mask_text)\n",
    "                    features_rgb.append(f)\n",
    "\n",
    "                if features_mode == 'texture_features' or features_mode == 'combined':\n",
    "                    f_wavelet = self.get_texture_features_by_blocks(image_gray[y:y+h, x:x+w], level, bins, mask_text=relative_mask_text)\n",
    "                    features_wavelet.append(f_wavelet)\n",
    "\n",
    "                if features_mode == 'keypoint':\n",
    "                    f_keypoints = self.get_features_by_keypoints(image_gray, keypoint_mode, n_features, mask_painting)\n",
    "                    features_keypoints.append(f_keypoints)\n",
    "            \n",
    "            # Append the features to the dict\n",
    "            if features_mode == 'texture_features':\n",
    "                processed_features[img_id] = features_wavelet\n",
    "            \n",
    "            elif features_mode == 'text_features':\n",
    "                processed_features[img_id] = features_text\n",
    "\n",
    "            elif features_mode == 'color_features':\n",
    "                processed_features[img_id] = features_rgb\n",
    "            \n",
    "            elif features_mode == 'combined':\n",
    "                if n_paintings > 1:\n",
    "                    assert len(features_rgb) == len(features_wavelet) == len(features_text), 'The number of features must be the same for each mode!'\n",
    "                    processed_features[img_id] = [[features_rgb[i], features_wavelet[i], features_text[i]] for i in range(n_paintings)]\n",
    "                else:\n",
    "                    processed_features[img_id] = [[features_rgb, features_wavelet, features_text]]\n",
    "            \n",
    "            elif features_mode == 'keypoint':\n",
    "                processed_features[img_id] = features_keypoints\n",
    "            \n",
    "        return processed_features, masks, masks_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader objects for both the database and the queries\n",
    "data_loader = DataLoader('data/BBDD')\n",
    "data_loader_qsd1_w4 = DataLoader('data/qsd1_w4')\n",
    "data_loader_qst1_w4 = DataLoader('data/qst1_w4')\n",
    "\n",
    "# Load ground truth files for each query\n",
    "with open('data/qsd1_w4/gt_corresps.pkl', 'rb') as f:\n",
    "    gt_w4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation results\n",
    "\n",
    "### Task 1, 2, 3 and 4: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mode = 'keypoint'\n",
    "keypoint_mode, sim_func = 'sift', cv2.NORM_L2\n",
    "n_features = 2048\n",
    "threshold = 190\n",
    "\n",
    "# Compute features for the database and the query images\n",
    "features, _, _ = data_loader.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=False)\n",
    "features_q1_w4, masks_query, text_boxes_query = data_loader_qsd1_w4.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=True, remove_text=True)\n",
    "\n",
    "result = utils.compare_keypoints(features_q1_w4, features, k, sim_func, threshold_matches=threshold)\n",
    "mapk_1 = utils.mapk(gt_w4, result, 1)\n",
    "mapk_k = utils.mapk(gt_w4, result, k)\n",
    "\n",
    "print(f'F1 score with threshold {threshold}:', utils.calculate_f1_score(result, gt_w4))\n",
    "print(f'Sift, L2, {n_features} features = mAP@1: {mapk_1}; mAP@{k}: {mapk_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mode = 'keypoint'\n",
    "keypoint_mode, sim_func = 'orb', cv2.NORM_HAMMING2\n",
    "n_features = 2048\n",
    "threshold = 100\n",
    "# Compute features for the database and the query images\n",
    "features, _, _ = data_loader.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=False)\n",
    "features_q1_w4, _, _ = data_loader_qsd1_w4.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=True, remove_text=True)\n",
    "\n",
    "result = utils.compare_keypoints(features_q1_w4, features, k, sim_func, threshold_matches=threshold)\n",
    "mapk_1 = utils.mapk(gt_w4, result, 1)\n",
    "mapk_k = utils.mapk(gt_w4, result, k)\n",
    "\n",
    "print(f'F1 score with threshold {threshold}:', utils.calculate_f1_score(result, gt_w4))\n",
    "print(f'Orb, Hamming, {n_features} features = mAP@1: {mapk_1}; mAP@{k}: {mapk_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mode = 'keypoint'\n",
    "keypoint_mode, sim_func = 'akaze', cv2.NORM_HAMMING2\n",
    "n_features = 512\n",
    "threshold = 40\n",
    "# Compute features for the database and the query images\n",
    "features, _, _ = data_loader.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=False)\n",
    "features_q1_w4, _, _ = data_loader_qsd1_w4.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=True, remove_text=True)\n",
    "\n",
    "result = utils.compare_keypoints(features_q1_w4, features, k, sim_func, threshold_matches=threshold)\n",
    "mapk_1 = utils.mapk(gt_w4, result, 1)\n",
    "mapk_k = utils.mapk(gt_w4, result, k)\n",
    "\n",
    "print(f'F1 score with threshold {threshold}:', utils.calculate_f1_score(result, gt_w4))\n",
    "print(f'Akaze, Hamming, {n_features} features = mAP@1: {mapk_1}; mAP@{k}: {mapk_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mode = 'combined'\n",
    "d_hist = 2\n",
    "level = 3\n",
    "bins = 8\n",
    "# Compute features for the database and the query images\n",
    "features, _, _ = data_loader.load_data(features_mode=features_mode, remove_background=False, level=level, d_hist=d_hist, bins=bins)\n",
    "features_q1_w4, _, _ = data_loader_qsd1_w4.load_data(features_mode=features_mode, remove_background=True, remove_text=True, level=level, d_hist=d_hist, bins=bins)\n",
    "\n",
    "# for thres in [x for x in np.arange(0.1,1,0.1)]: # best is 3.5\n",
    "result = utils.compare_images(features_q1_w4, features, 5, utils.histogram_intersection, combine=True, param=[4,2,1], threshold_dist=3.5)\n",
    "mapk_1 = utils.mapk(gt_w4, result, 1)\n",
    "mapk_k = utils.mapk(gt_w4, result, k)\n",
    "\n",
    "print(f'F1 score with threshold {3.5}:', utils.calculate_f1_score(result, gt_w4))\n",
    "print(f'Combined param {[4,2,1]}, histogram_intersection = \\tmAP@{1}: {mapk_1}, mAP@{k}: {mapk_k}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
