{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1 - Content Based Image Retrieval\n",
    "### Team 8 - Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, glob, math, tqdm, pickle, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import pytesseract\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "import pywt\n",
    "from skimage.feature import local_binary_pattern\n",
    "from scipy.fftpack import dctn\n",
    "\n",
    "import utils\n",
    "\n",
    "#autoreload modules when code is run\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Path to the OCR executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Luis\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set to store unique names of authors.\n",
    "name_bag = set()\n",
    "\n",
    "for folder in ['BBDD', 'qsd1_w4']:\n",
    "    # Loop through each .txt file inside the folder.\n",
    "    for text_file in glob.glob(f'data/{folder}/*.txt'):\n",
    "        # Extract the specific text pattern from the file and add it to the set.\n",
    "        name_bag.add(utils.get_text_bbdd(text_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "\n",
    "    # Divide the image into blocks\n",
    "    def create_blocks_array(self, image, blockNumber):\n",
    "        # Set number of slices per axis\n",
    "        axisSlice = int(math.sqrt(blockNumber))\n",
    "\n",
    "        blocksArray = []\n",
    "        # Split the image into vertical blocks\n",
    "        split_h = np.array_split(image, axisSlice, axis = 0)\n",
    "        \n",
    "        for i in range(axisSlice):\n",
    "            for j in range(axisSlice):\n",
    "                # Split vertical blocks into square blocks\n",
    "                split_hv = np.array_split(split_h[i], axisSlice, axis = 1)\n",
    "                blocksArray.append(split_hv[j])\n",
    "        return blocksArray\n",
    "\n",
    "    # Compute the histogram of the image\n",
    "    def create_histogram(self, block, mask, d_hist, bins):\n",
    "        \n",
    "        channels = cv2.split(block)\n",
    "        range_a, range_b = 256, 256\n",
    "\n",
    "        if d_hist == 1:\n",
    "            if mask is None:\n",
    "                # Compute 1D histograms for each channel separately\n",
    "                hist = [cv2.calcHist([chan], [0], None, [bins], [0, range_a if i == 0 else range_b]) for i,chan in enumerate(channels)]\n",
    "            else:\n",
    "                # Compute 1D histograms for each channel separately\n",
    "                hist = [cv2.calcHist([chan[mask!=0]], [0], None, [bins], [0, range_a if i == 0 else range_b]) for i,chan in enumerate(channels)]\n",
    "\n",
    "        elif d_hist == 2:\n",
    "            if mask is None:\n",
    "                # Compute 2D joint histograms for each pair of channels\n",
    "                hist = [cv2.calcHist([channels[i], channels[j]], [0, 1], None, [bins, bins], [0, range_a if i == 0 else range_b, 0, range_b])\n",
    "                            for i in range(len(channels)) for j in range(i+1, len(channels))]\n",
    "            else:\n",
    "                # Compute 2D joint histograms for each pair of channels\n",
    "                hist = [cv2.calcHist([channels[i][mask!=0], channels[j][mask!=0]], [0, 1], None, [bins, bins], [0, range_a if i == 0 else range_b, 0, range_b])\n",
    "                            for i in range(len(channels)) for j in range(i+1, len(channels))]\n",
    "\n",
    "        else:\n",
    "            if mask is None:\n",
    "                # Compute 3D joint histogram for all three channels\n",
    "                hist, _ = np.histogramdd([c.flatten() for c in channels], bins=(bins, bins, bins), range=[(0, range_a), (0, range_b), (0, range_b)])\n",
    "            else:\n",
    "                # Compute 3D joint histogram for all three channels\n",
    "                hist, _ = np.histogramdd([c[mask != 0] for c in channels], bins=(bins, bins, bins), range=[(0, range_a), (0, range_b), (0, range_b)])\n",
    "\n",
    "        return hist\n",
    "    \n",
    "    # Compute the color histogram of the image by blocks\n",
    "    def get_color_features_by_blocks(self, image, level, d_hist, bins, mask_text):\n",
    "\n",
    "        # Get blocks using multi-level resolution\n",
    "        blocksArray = []\n",
    "        for lvl in range(level+1):\n",
    "            for b in self.create_blocks_array(image, (2**lvl)*(2**lvl)):\n",
    "                blocksArray.append(b)\n",
    "\n",
    "        if mask_text is not None:\n",
    "            blocksMasks = []\n",
    "\n",
    "            # We create a mask image blocking the bbox of the text\n",
    "            # That image will be used to compute the histogram of the image without the text\n",
    "            mask_text_image = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "            mask_text_image[mask_text[1]:mask_text[3], mask_text[0]:mask_text[2]] = 0\n",
    "\n",
    "            # It is necessary to create the blocks of the mask image too\n",
    "            for lvl in range(level+1):\n",
    "                for b in self.create_blocks_array(mask_text_image, (2**lvl)*(2**lvl)):\n",
    "                    blocksMasks.append(b)\n",
    "        else:\n",
    "            blocksMasks = [None]*len(blocksArray)\n",
    "\n",
    "        histograms = []\n",
    "        for block, mask_text_block in zip(blocksArray, blocksMasks):\n",
    "            # Compute the histogram of the channel and append it to the list\n",
    "            hist = self.create_histogram(block, mask_text_block, d_hist, bins)\n",
    "            if isinstance(hist, list):\n",
    "                for h in hist:\n",
    "                    histograms.append(h.flatten() / (block.shape[0]*block.shape[1]))\n",
    "            else:\n",
    "                histograms.append(hist.flatten()  / (block.shape[0]*block.shape[1]))\n",
    "            \n",
    "        # Concatenate all histograms into a single feature vector\n",
    "        return np.concatenate(histograms)\n",
    "\n",
    "    def zigzag_scan(self, image):\n",
    "        rows, cols = image.shape\n",
    "        solution = [[] for _ in range(rows + cols - 1)]\n",
    "        \n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                sum_idx = i + j\n",
    "                if (sum_idx % 2 == 0):\n",
    "                    # add at beginning if even index\n",
    "                    solution[sum_idx].insert(0, image[i,j])\n",
    "                else:\n",
    "                    # add at the end if odd index\n",
    "                    solution[sum_idx].append(image[i,j])\n",
    "\n",
    "        # flatten the result\n",
    "        result = np.array([num for sublist in solution for num in sublist])\n",
    "        return result\n",
    "\n",
    "    # Compute different texture features by blocks\n",
    "    def get_texture_features_by_blocks(self, image, level, bins, mask_text):\n",
    "        \n",
    "        # Get blocks using multi-level resolution\n",
    "        blocksArray = []\n",
    "        for lvl in range(level+1):\n",
    "            for b in self.create_blocks_array(image, (2**lvl)*(2**lvl)):\n",
    "                blocksArray.append(b)\n",
    "\n",
    "        if mask_text is not None:\n",
    "            blocksMasks = []\n",
    "\n",
    "            # We create a mask image blocking the bbox of the text\n",
    "            # That image will be used to compute the histogram of the image without the text\n",
    "            mask_text_image = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "            # Assign zero to the region corresponding to the text\n",
    "            mask_text_image[mask_text[1]:mask_text[3], mask_text[0]:mask_text[2]] = 0\n",
    "\n",
    "            # It is necessary to create the blocks of the mask image too\n",
    "            for lvl in range(level+1):\n",
    "                for b in self.create_blocks_array(mask_text_image, (2**lvl)*(2**lvl)):\n",
    "                    blocksMasks.append(b)\n",
    "        else:\n",
    "            blocksMasks = [None]*len(blocksArray)\n",
    "\n",
    "        histograms = []\n",
    "        # For each block and its corresponding text mask block, compute texture features\n",
    "        for block, mask_text_block in zip(blocksArray, blocksMasks):\n",
    "            \n",
    "            details = pywt.dwt2(block, 'bior1.3')\n",
    "            approx, (h, v, d) = details # approx captures bigger details (more smooth than the original img), (h, v, d) capture de horizontal, vertical and diagonal \"smaller\" details\n",
    "            \n",
    "            if mask_text_block is not None:\n",
    "                # Resize the text mask to the size of the wavelet's resulting images\n",
    "                new_mask = cv2.resize(mask_text_block, approx.shape[::-1]).astype(bool) \n",
    "\n",
    "            # Create an histogram for each wavelet \"image\" and concatenate all of them\n",
    "            final_hist = []\n",
    "            for wt_img in [approx, h, v, d]:\n",
    "                hist = np.histogram(wt_img if mask_text_block is None else wt_img[new_mask != 0], bins=bins, range=(0, 256))[0]\n",
    "                final_hist.append(hist.flatten() / (wt_img.shape[0]*wt_img.shape[1]))\n",
    "            histograms.append(np.concatenate(final_hist))\n",
    "            \n",
    "        # Concatenate all histograms into a single feature vector\n",
    "        return np.concatenate(histograms)\n",
    "\n",
    "    def get_features_by_keypoints(self, gray, mode, n_features, mask):\n",
    "        \n",
    "        if mode == 'sift':\n",
    "            # SIFT Detector\n",
    "            sift = cv2.SIFT_create(nfeatures=n_features)\n",
    "            _, des = sift.detectAndCompute(gray, mask)\n",
    "\n",
    "        elif mode == 'orb':\n",
    "            # ORB Detector\n",
    "            orb = cv2.ORB_create(nfeatures=n_features)\n",
    "            _, des = orb.detectAndCompute(gray, mask)\n",
    "\n",
    "        elif mode == 'akaze':\n",
    "            thres = 0.005\n",
    "            # AKAZE Detector\n",
    "            akaze = cv2.AKAZE_create(threshold=thres)\n",
    "            _, des = akaze.detectAndCompute(gray, mask)\n",
    "\n",
    "            while des is None or des.shape[0] < n_features:\n",
    "                if str(thres)[-1] == '1': \n",
    "                    thres = thres / 2\n",
    "                else:\n",
    "                    thres /= 5\n",
    "                \n",
    "                akaze = cv2.AKAZE_create(threshold=thres)\n",
    "                _, des = akaze.detectAndCompute(gray, mask)\n",
    "\n",
    "                if thres < 1e-6:\n",
    "                    break\n",
    "\n",
    "        return des\n",
    "\n",
    "    def clean_noise(self, image, k):\n",
    "        return cv2.medianBlur(image, k)\n",
    "    \n",
    "    # Load data, calculate background and text masks (if necessary) and compute features\n",
    "    def load_data(self, level = 3, d_hist = 1, bins = 8, n_features=2048, keypoint_mode='sift', remove_background=False, remove_text=False, features_mode='color_features'):\n",
    "        # Get a list of all image file names in the folder\n",
    "        image_files = sorted(glob.glob(self.folder_path+'/*.jpg'))\n",
    "\n",
    "        # Initialize an empty list to store the processed images and masks\n",
    "        processed_features = dict()\n",
    "        masks, masks_text = [], []\n",
    "\n",
    "        # Iterate over each image file\n",
    "        for f in tqdm.tqdm(image_files):\n",
    "            \n",
    "            # Get the image id from the file name. Depending on the OS, the path separator is different\n",
    "            try:\n",
    "                img_id = int(f.split('\\\\')[-1].split('.')[0].split('_')[-1])\n",
    "            except:\n",
    "                img_id = int(f.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "\n",
    "            # Load the image in BGR format\n",
    "            image = cv2.imread(f)\n",
    "\n",
    "            # Clean noise of the image using median filter\n",
    "            image = self.clean_noise(image, k=3)\n",
    "\n",
    "            # Convert the image to grayscale\n",
    "            image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            features_rgb, features_wavelet, features_text, features_keypoints = [], [], [], []\n",
    "\n",
    "            # Remove background (there can be 2 paintings in the same image)\n",
    "            if remove_background:\n",
    "                mask_image, coordinates = utils.get_mask(image_gray)\n",
    "                n_paintings = len(coordinates)\n",
    "               \n",
    "                # Remove the text from each image\n",
    "                if remove_text:\n",
    "                    coordinates = sorted(coordinates, key=lambda x: (x[0], x[1]))\n",
    "                    masks_text_i = [[None]]*n_paintings\n",
    "                    # coordinates contains the coordinates of the paintings mask in the image\n",
    "                    # We iterate over each masked painting and get the text mask for each one\n",
    "                    # We hace to recover the original coordinates for the text mask\n",
    "                    for i, (x,y,w,h) in enumerate(coordinates):\n",
    "                        x_text, y_text, x_text_max, y_text_max, text = utils.get_mask_text(image_gray[y:y+h, x:x+w], name_bag=name_bag)\n",
    "                        features_text.append(text)\n",
    "                        masks_text_i[i] = [x+x_text, y+y_text, x+x_text_max, y+y_text_max]\n",
    "                    masks_text.append(masks_text_i)\n",
    "                else:\n",
    "                    masks_text.extend([[None] for _ in range(n_paintings)])\n",
    "\n",
    "            else:\n",
    "                mask_image = None\n",
    "\n",
    "                # if there is no background, the mask is the whole image\n",
    "                coordinates = [[0,0,image.shape[1],image.shape[0]]]\n",
    "                n_paintings = 1\n",
    "                if remove_text:\n",
    "                    x_text, y_text, x_text_max, y_text_max, text = utils.get_mask_text(image_gray, name_bag=name_bag)\n",
    "                    features_text.append(text)\n",
    "                    masks_text.append([[x_text, y_text, x_text_max, y_text_max]])\n",
    "                else:\n",
    "                    masks_text.append([None])\n",
    "\n",
    "            masks.append(mask_image)\n",
    "        \n",
    "            for i in range(n_paintings):\n",
    "                x,y,w,h = coordinates[i]\n",
    "\n",
    "                mask_painting = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "                mask_painting[y:y+h, x:x+w] = 255\n",
    "\n",
    "                relative_mask_text = None\n",
    "                if masks_text[-1][i] is not None:\n",
    "                    relative_mask_text = [masks_text[-1][i][0]-x, masks_text[-1][i][1]-y, masks_text[-1][i][2]-x, masks_text[-1][i][3]-y]\n",
    "                    # Remove the text from the image \n",
    "                    mask_painting[masks_text[-1][i][1]:masks_text[-1][i][3], masks_text[-1][i][0]:masks_text[-1][i][2]] = 0\n",
    "\n",
    "                if features_mode == 'color_features' or features_mode == 'combined':    \n",
    "                    # Get the features of every masked image\n",
    "                    f = self.get_color_features_by_blocks(image[y:y+h, x:x+w], level, d_hist, bins, mask_text=relative_mask_text)\n",
    "                    features_rgb.append(f)\n",
    "\n",
    "                if features_mode == 'texture_features' or features_mode == 'combined':\n",
    "                    f_wavelet = self.get_texture_features_by_blocks(image_gray[y:y+h, x:x+w], level, bins, mask_text=relative_mask_text)\n",
    "                    features_wavelet.append(f_wavelet)\n",
    "\n",
    "                if features_mode == 'keypoint':\n",
    "                    f_keypoints = self.get_features_by_keypoints(image_gray, keypoint_mode, n_features, mask_painting)\n",
    "                    features_keypoints.append(f_keypoints)\n",
    "            \n",
    "            # Append the features to the dict\n",
    "            if features_mode == 'texture_features':\n",
    "                processed_features[img_id] = features_wavelet\n",
    "            \n",
    "            elif features_mode == 'text_features':\n",
    "                processed_features[img_id] = features_text\n",
    "\n",
    "            elif features_mode == 'color_features':\n",
    "                processed_features[img_id] = features_rgb\n",
    "            \n",
    "            elif features_mode == 'combined':\n",
    "                if n_paintings > 1:\n",
    "                    assert len(features_rgb) == len(features_wavelet) == len(features_text), 'The number of features must be the same for each mode!'\n",
    "                    processed_features[img_id] = [[features_rgb[i], features_wavelet[i], features_text[i]] for i in range(n_paintings)]\n",
    "                else:\n",
    "                    processed_features[img_id] = [[features_rgb, features_wavelet, features_text]]\n",
    "            \n",
    "            elif features_mode == 'keypoint':\n",
    "                processed_features[img_id] = features_keypoints\n",
    "            \n",
    "        return processed_features, masks, masks_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader objects for both the database and the queries\n",
    "data_loader = DataLoader('data/BBDD')\n",
    "data_loader_qsd1_w4 = DataLoader('data/qsd1_w4')\n",
    "\n",
    "# Load ground truth files for each query\n",
    "with open('data/qsd1_w4/gt_corresps.pkl', 'rb') as f:\n",
    "    gt_w4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation results\n",
    "\n",
    "### Task 1, 2, 3 and 4: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing matches: 100%|██████████| 30/30 [29:48<00:00, 59.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with threshold 90: 0.8656716417910447\n",
      "Sift, L2, 2048 features = mAP@1: 0.7368421052631579; mAP@5: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing matches: 100%|██████████| 30/30 [28:31<00:00, 57.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with threshold 190: 0.9142857142857143\n",
      "Sift, L2, 2048 features = mAP@1: 0.8157894736842105; mAP@5: 0.8289473684210527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing matches: 100%|██████████| 30/30 [29:05<00:00, 58.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with threshold 290: 0.9295774647887324\n",
      "Sift, L2, 2048 features = mAP@1: 0.8421052631578947; mAP@5: 0.8552631578947368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features_mode = 'keypoint'\n",
    "keypoint_mode, sim_func = 'sift', cv2.NORM_L1\n",
    "n_features = 2048\n",
    "# threshold = 190\n",
    "\n",
    "# Compute features for the database and the query images\n",
    "# features, _, _ = data_loader.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=False)\n",
    "# features_q1_w4, _, _ = data_loader_qsd1_w4.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=True, remove_text=True)\n",
    "\n",
    "for threshold in [90, 190, 290]:\n",
    "    result = utils.compare_keypoints(features_q1_w4, features, k, sim_func, threshold_matches=threshold)\n",
    "    mapk_1 = utils.mapk(gt_w4, result, 1)\n",
    "    mapk_k = utils.mapk(gt_w4, result, k)\n",
    "\n",
    "    print(f'F1 score with threshold {threshold}:', utils.calculate_f1_score(result, gt_w4))\n",
    "    print(f'Sift, L2, {n_features} features = mAP@1: {mapk_1}; mAP@{k}: {mapk_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing matches: 100%|██████████| 30/30 [10:32<00:00, 21.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, [(135, 13), (113, 11), (43, 10), (156, 10), (194, 10)])], [(1, [(104, 101), (86, 15), (164, 15), (211, 15), (267, 15)])], [(2, [(9, 11), (78, 11), (193, 11), (7, 10), (15, 10)])], [(3, [(113, 18), (164, 17), (33, 16), (267, 16), (196, 14)])], [(4, [(130, 15), (236, 15), (4, 14), (74, 13), (118, 13)])], [(5, [(132, 10), (164, 10), (45, 9), (118, 9), (30, 8)]), (5, [(164, 25), (94, 12), (267, 12), (194, 11), (58, 10)]), (5, [(8, 13), (116, 13), (72, 12), (113, 12), (34, 11)])], [(6, [(39, 3), (206, 3), (209, 3), (250, 3), (25, 2)])], [(7, [(41, 104), (97, 11), (85, 10), (266, 10), (5, 9)])], [(8, [(104, 177), (164, 16), (231, 13), (265, 12), (5, 11)])], [(9, [(60, 11), (124, 11), (109, 10), (8, 9), (30, 8)])], [(10, [(35, 191), (233, 13), (12, 12), (29, 12), (78, 11)])], [(11, [(286, 38), (164, 15), (200, 14), (271, 13), (7, 12)])], [(12, [(35, 10), (170, 10), (159, 9), (260, 9), (17, 8)])], [(13, [(12, 156), (85, 12), (255, 12), (239, 11), (5, 10)])], [(14, [(164, 12), (93, 8), (75, 7), (150, 7), (156, 7)])], [(15, [(184, 5), (222, 5), (69, 3), (198, 3), (251, 3)]), (15, [(182, 15), (53, 10), (225, 9), (42, 8), (115, 8)]), (15, [(252, 12), (226, 7), (133, 6), (179, 6), (183, 6)])], [(16, [(260, 128), (164, 19), (61, 17), (113, 15), (102, 13)]), (16, [(221, 9), (23, 7), (117, 7), (182, 7), (192, 7)])], [(17, [(164, 4), (80, 3), (266, 3), (270, 3), (5, 2)])], [(18, [(7, 15), (92, 15), (120, 15), (137, 15), (270, 15)])], [(19, [(251, 36), (107, 6), (259, 6), (42, 5), (47, 5)])], [(20, [(228, 15), (63, 14), (248, 14), (99, 13), (257, 13)])], [(21, [(190, 11), (71, 9), (167, 9), (212, 9), (23, 8)])], [(22, [(32, 10), (164, 10), (160, 7), (239, 7), (280, 7)])], [(23, [(199, 30), (133, 10), (205, 9), (261, 9), (10, 8)]), (23, [(34, 60), (164, 12), (122, 11), (71, 10), (114, 10)])], [(24, [(193, 130), (248, 13), (179, 11), (222, 11), (28, 10)])], [(25, [(163, 113), (164, 17), (72, 12), (3, 11), (137, 11)])], [(26, [(117, 11), (31, 8), (84, 8), (100, 8), (141, 7)]), (26, [(192, 9), (269, 6), (278, 6), (5, 5), (7, 5)])], [(27, [(164, 24), (51, 12), (78, 10), (44, 8), (54, 8)])], [(28, [(93, 86), (164, 5), (101, 4), (141, 4), (202, 4)])], [(29, [(116, 85), (164, 17), (40, 13), (255, 13), (54, 12)]), (29, [(147, 15), (97, 12), (200, 12), (5, 11), (17, 11)])]]\n",
      "F1 score with threshold 100: 0.7741935483870968\n",
      "Orb, Hamming, 2048 features = mAP@1: 0.5526315789473685; mAP@5: 0.5789473684210527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features_mode = 'keypoint'\n",
    "keypoint_mode, sim_func = 'orb', cv2.NORM_L2 \n",
    "n_features = 2048\n",
    "threshold = 100\n",
    "# Compute features for the database and the query images\n",
    "features, _, _ = data_loader.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=False)\n",
    "features_q1_w4, _, _ = data_loader_qsd1_w4.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=True, remove_text=True)\n",
    "\n",
    "result = utils.compare_keypoints(features_q1_w4, features, k, sim_func, threshold_matches=threshold)\n",
    "mapk_1 = utils.mapk(gt_w4, result, 1)\n",
    "mapk_k = utils.mapk(gt_w4, result, k)\n",
    "\n",
    "print(f'F1 score with threshold {threshold}:', utils.calculate_f1_score(result, gt_w4))\n",
    "print(f'Orb, Hamming, {n_features} features = mAP@1: {mapk_1}; mAP@{k}: {mapk_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286/286 [04:49<00:00,  1.01s/it]\n",
      "100%|██████████| 30/30 [01:09<00:00,  2.33s/it]\n",
      "Computing matches: 100%|██████████| 30/30 [08:25<00:00, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with threshold 40: 0.8484848484848485\n",
      "Akaze, Hamming, 100 features = mAP@1: 0.7368421052631579; mAP@5: 0.7368421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features_mode = 'keypoint'\n",
    "keypoint_mode, sim_func = 'akaze', cv2.NORM_L2\n",
    "n_features = 100\n",
    "threshold = 40\n",
    "# Compute features for the database and the query images\n",
    "features, _, _ = data_loader.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=False)\n",
    "features_q1_w4, _, _ = data_loader_qsd1_w4.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=True, remove_text=True)\n",
    "\n",
    "result = utils.compare_keypoints(features_q1_w4, features, k, sim_func, threshold_matches=threshold)\n",
    "mapk_1 = utils.mapk(gt_w4, result, 1)\n",
    "mapk_k = utils.mapk(gt_w4, result, k)\n",
    "\n",
    "print(f'F1 score with threshold {threshold}:', utils.calculate_f1_score(result, gt_w4))\n",
    "print(f'Akaze, Hamming, {n_features} features = mAP@1: {mapk_1}; mAP@{k}: {mapk_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mode = 'combined'\n",
    "d_hist = 2\n",
    "level = 3\n",
    "bins = 8\n",
    "# Compute features for the database and the query images\n",
    "features, _, _ = data_loader.load_data(features_mode=features_mode, remove_background=False, level=level, d_hist=d_hist, bins=bins)\n",
    "features_q1_w4, _, _ = data_loader_qsd1_w4.load_data(features_mode=features_mode, remove_background=True, remove_text=True, level=level, d_hist=d_hist, bins=bins)\n",
    "\n",
    "for thres in [x for x in np.arange(0.5,5.5,0.5)]:\n",
    "    result = utils.compare_images(features_q1_w4, features, 5, utils.histogram_intersection, combine=True, param=[4,2,1], threshold_dist=3.5)\n",
    "    mapk_1 = utils.mapk(gt_w4, result, 1)\n",
    "    mapk_k = utils.mapk(gt_w4, result, k)\n",
    "\n",
    "    print(f'F1 score with threshold {thres}:', utils.calculate_f1_score(result, gt_w4))\n",
    "    print(f'Combined param {[4,2,1]}, histogram_intersection = \\tmAP@{1}: {mapk_1}, mAP@{k}: {mapk_k}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
