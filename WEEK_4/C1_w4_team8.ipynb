{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1 - Content Based Image Retrieval\n",
    "### Team 8 - Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, glob, math, tqdm, pickle, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import pytesseract\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "import pywt\n",
    "from skimage.feature import local_binary_pattern\n",
    "from scipy.fftpack import dctn\n",
    "\n",
    "import utils\n",
    "\n",
    "#autoreload modules when code is run\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Path to the OCR executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Luis\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set to store unique names of authors.\n",
    "name_bag = set()\n",
    "\n",
    "for folder in ['BBDD', 'qsd1_w4']:\n",
    "    # Loop through each .txt file inside the folder.\n",
    "    for text_file in glob.glob(f'data/{folder}/*.txt'):\n",
    "        # Extract the specific text pattern from the file and add it to the set.\n",
    "        name_bag.add(utils.get_text_bbdd(text_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickle file 'data/qsd1_w4/text_boxes.pkl'\n",
    "with open('data/qsd1_w4/text_boxes.pkl', 'rb') as f:\n",
    "    text_boxes = pickle.load(f)\n",
    "\n",
    "# load pickle file 'data/qsd1_w4/frames.pkl'\n",
    "with open('data/qsd1_w4/frames.pkl', 'rb') as f:\n",
    "    frames = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "\n",
    "    # Obtain the painting image removing the background. \n",
    "    # It returns the mask where 1 means painting image and 0 background.\n",
    "    def get_mask(self, gray, threshold_area=65000):\n",
    "     \n",
    "        # Empty mask definition\n",
    "        mask = np.zeros(gray.shape, dtype=np.uint8)\n",
    "\n",
    "        # Applying gaussian blurring and define an intelligent gradient threshold depending on 13x13 boxes\n",
    "        blur = cv2.GaussianBlur(gray, (13,13), 0)\n",
    "        # Threshold based on local pixel neighborhood (11x11 block size)\n",
    "        thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "        # Two pass dilate with horizontal and vertical kernel\n",
    "        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,5))\n",
    "        dilate = cv2.dilate(thresh, horizontal_kernel, iterations=2)\n",
    "        vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,1))\n",
    "        dilate = cv2.dilate(dilate, vertical_kernel, iterations=2)\n",
    "\n",
    "        # Find contours, filter using contour threshold area, and draw rectangle\n",
    "        cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "        # Filtering the found contours by size\n",
    "        counter = 0\n",
    "        areas = []\n",
    "        coordinates = []\n",
    "        for c in cnts:\n",
    "            # Shoelace formula for convex shapes\n",
    "            area = cv2.contourArea(c) \n",
    "            if area > threshold_area:\n",
    "                x,y,w,h = cv2.boundingRect(c) \n",
    "                areas.append((area, (x,y,w,h)))\n",
    "                counter += 1\n",
    "\n",
    "        # Sort areas and positions by area\n",
    "        areas = sorted(areas, key=lambda x: x[0], reverse=True)[:3]\n",
    "\n",
    "        # Draw bounding box on mask\n",
    "        for i in range(len(areas)-1,-1,-1):\n",
    "            if i > 0 and abs(areas[i][1][0] - areas[i-1][1][0]) < 190 and abs(areas[i][1][1] - areas[i-1][1][1]) < 150:\n",
    "                # print('Skipping! Two masks in the same painting!')\n",
    "                continue\n",
    "            x,y,w,h = areas[i][1]\n",
    "            coordinates.append((x,y,w,h))\n",
    "            mask[y:y+h, x:x+w] = 255\n",
    "        \n",
    "        # Catching the 0 contours error\n",
    "        if counter == 0:\n",
    "            print('Error! No paintings in this image!')\n",
    "            plt.imshow(gray)\n",
    "            plt.show()\n",
    "            plt.imshow(mask, cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "        return mask, coordinates\n",
    "    \n",
    "    # Obtain the painting image removing the text. \n",
    "    # It returns the mask where 1 means painting image and 0 text.\n",
    "    def get_mask_text(self, gray, name_bag):\n",
    "\n",
    "        # Apply morphological opening and closing to enhance text-like features using a 9x9 kernel\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9,9))\n",
    "        opening = cv2.morphologyEx(gray, cv2.MORPH_OPEN, kernel)\n",
    "        closing = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        #thresholding the difference to get (hopefully) only the text\n",
    "        x = closing-opening\n",
    "        x = (x>125).astype(np.uint8) \n",
    "\n",
    "        # Dilation to further enhance the text features using a 13x13 kernel\n",
    "        kernel2 = cv2.getStructuringElement(cv2.MORPH_RECT, (13,13))\n",
    "        dilated = cv2.dilate(x, kernel2, iterations=2)\n",
    "\n",
    "        # Find contours \n",
    "        ctns = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Loop through the contours and find rectangular bounding boxes that likely represent text areas\n",
    "        areas = []\n",
    "        for c in ctns[0]:\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            # Filter out rectangles based on certain geometric criteria\n",
    "            n, m = gray.shape\n",
    "            ratio = w/h\n",
    "            relative_area = (w*h)/(n*m)\n",
    "            if w > h and ratio < 12 and ratio > 1.5 and relative_area < 0.25 and x+w < m and y+h < n and h >= 30:\n",
    "                # Shoelace formula for convex shapes\n",
    "                areas.append((cv2.contourArea(c), (x,y,w,h)))\n",
    "        \n",
    "        if len(areas) == 0:\n",
    "            return 0, 0, 0, 0, 'Unknown'\n",
    "        areas = sorted(areas, key=lambda x: x[0], reverse=True)\n",
    "        x, y, w, h = areas[0][1]\n",
    "\n",
    "        # Merge shapes close to the main detected text region (e.g., text broken into separate regions)\n",
    "        for _, shape in areas:\n",
    "            if y > shape[1]-10 and y < shape[1]+10:\n",
    "                if shape[0] < x:\n",
    "                    w = (x+w) - shape[0]\n",
    "                    x = shape[0]\n",
    "                else:\n",
    "                    w = (shape[0]+shape[2]) - x\n",
    "\n",
    "\n",
    "        min_word = utils.get_text(gray, name_bag, x, y, x+w, y+h)\n",
    "        # Return the bounding box of the detected text region and the closest matching name\n",
    "        return [x, y, x+w, y+h, min_word]\n",
    "\n",
    "    # Divide the image into blocks\n",
    "    def create_blocks_array(self, image, blockNumber):\n",
    "    \n",
    "        # Set number of slices per axis\n",
    "        axisSlice = int(math.sqrt(blockNumber))\n",
    "\n",
    "        blocksArray = []\n",
    "        # Split the image into vertical blocks\n",
    "        split_h = np.array_split(image, axisSlice, axis = 0)\n",
    "        \n",
    "        for i in range(axisSlice):\n",
    "            for j in range(axisSlice):\n",
    "                # Split vertical blocks into square blocks\n",
    "                split_hv = np.array_split(split_h[i], axisSlice, axis = 1)\n",
    "                blocksArray.append(split_hv[j])\n",
    "        return blocksArray\n",
    "\n",
    "    # Compute the histogram of the image\n",
    "    def create_histogram(self, block, mask, d_hist, bins):\n",
    "        \n",
    "        channels = cv2.split(block)\n",
    "        range_a, range_b = 256, 256\n",
    "\n",
    "        if d_hist == 1:\n",
    "            if mask is None:\n",
    "                # Compute 1D histograms for each channel separately\n",
    "                hist = [cv2.calcHist([chan], [0], None, [bins], [0, range_a if i == 0 else range_b]) for i,chan in enumerate(channels)]\n",
    "            else:\n",
    "                # Compute 1D histograms for each channel separately\n",
    "                hist = [cv2.calcHist([chan[mask!=0]], [0], None, [bins], [0, range_a if i == 0 else range_b]) for i,chan in enumerate(channels)]\n",
    "\n",
    "        elif d_hist == 2:\n",
    "            if mask is None:\n",
    "                # Compute 2D joint histograms for each pair of channels\n",
    "                hist = [cv2.calcHist([channels[i], channels[j]], [0, 1], None, [bins, bins], [0, range_a if i == 0 else range_b, 0, range_b])\n",
    "                            for i in range(len(channels)) for j in range(i+1, len(channels))]\n",
    "            else:\n",
    "                # Compute 2D joint histograms for each pair of channels\n",
    "                hist = [cv2.calcHist([channels[i][mask!=0], channels[j][mask!=0]], [0, 1], None, [bins, bins], [0, range_a if i == 0 else range_b, 0, range_b])\n",
    "                            for i in range(len(channels)) for j in range(i+1, len(channels))]\n",
    "\n",
    "        else:\n",
    "            if mask is None:\n",
    "                # Compute 3D joint histogram for all three channels\n",
    "                hist, _ = np.histogramdd([c.flatten() for c in channels], bins=(bins, bins, bins), range=[(0, range_a), (0, range_b), (0, range_b)])\n",
    "            else:\n",
    "                # Compute 3D joint histogram for all three channels\n",
    "                hist, _ = np.histogramdd([c[mask != 0] for c in channels], bins=(bins, bins, bins), range=[(0, range_a), (0, range_b), (0, range_b)])\n",
    "\n",
    "        return hist\n",
    "    \n",
    "    # Compute the color histogram of the image by blocks\n",
    "    def get_color_features_by_blocks(self, image, level, d_hist, bins, mask_text):\n",
    "\n",
    "        # Get blocks using multi-level resolution\n",
    "        blocksArray = []\n",
    "        for lvl in range(level+1):\n",
    "            for b in self.create_blocks_array(image, (2**lvl)*(2**lvl)):\n",
    "                blocksArray.append(b)\n",
    "\n",
    "        if mask_text is not None:\n",
    "            blocksMasks = []\n",
    "\n",
    "            # We create a mask image blocking the bbox of the text\n",
    "            # That image will be used to compute the histogram of the image without the text\n",
    "            mask_text_image = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "            mask_text_image[mask_text[1]:mask_text[3], mask_text[0]:mask_text[2]] = 0\n",
    "\n",
    "            # It is necessary to create the blocks of the mask image too\n",
    "            for lvl in range(level+1):\n",
    "                for b in self.create_blocks_array(mask_text_image, (2**lvl)*(2**lvl)):\n",
    "                    blocksMasks.append(b)\n",
    "        else:\n",
    "            blocksMasks = [None]*len(blocksArray)\n",
    "\n",
    "        histograms = []\n",
    "        for block, mask_text_block in zip(blocksArray, blocksMasks):\n",
    "            # Compute the histogram of the channel and append it to the list\n",
    "            hist = self.create_histogram(block, mask_text_block, d_hist, bins)\n",
    "            if isinstance(hist, list):\n",
    "                for h in hist:\n",
    "                    histograms.append(h.flatten() / (block.shape[0]*block.shape[1]))\n",
    "            else:\n",
    "                histograms.append(hist.flatten()  / (block.shape[0]*block.shape[1]))\n",
    "            \n",
    "        # Concatenate all histograms into a single feature vector\n",
    "        return np.concatenate(histograms)\n",
    "\n",
    "    def zigzag_scan(self, image):\n",
    "        rows, cols = image.shape\n",
    "        solution = [[] for _ in range(rows + cols - 1)]\n",
    "        \n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                sum_idx = i + j\n",
    "                if (sum_idx % 2 == 0):\n",
    "                    # add at beginning if even index\n",
    "                    solution[sum_idx].insert(0, image[i,j])\n",
    "                else:\n",
    "                    # add at the end if odd index\n",
    "                    solution[sum_idx].append(image[i,j])\n",
    "\n",
    "        # flatten the result\n",
    "        result = np.array([num for sublist in solution for num in sublist])\n",
    "        return result\n",
    "\n",
    "    # Compute different texture features by blocks\n",
    "    def get_texture_features_by_blocks(self, image, level, bins, mask_text):\n",
    "        \n",
    "        # Get blocks using multi-level resolution\n",
    "        blocksArray = []\n",
    "        for lvl in range(level+1):\n",
    "            for b in self.create_blocks_array(image, (2**lvl)*(2**lvl)):\n",
    "                blocksArray.append(b)\n",
    "\n",
    "        if mask_text is not None:\n",
    "            blocksMasks = []\n",
    "\n",
    "            # We create a mask image blocking the bbox of the text\n",
    "            # That image will be used to compute the histogram of the image without the text\n",
    "            mask_text_image = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "            # Assign zero to the region corresponding to the text\n",
    "            mask_text_image[mask_text[1]:mask_text[3], mask_text[0]:mask_text[2]] = 0\n",
    "\n",
    "            # It is necessary to create the blocks of the mask image too\n",
    "            for lvl in range(level+1):\n",
    "                for b in self.create_blocks_array(mask_text_image, (2**lvl)*(2**lvl)):\n",
    "                    blocksMasks.append(b)\n",
    "        else:\n",
    "            blocksMasks = [None]*len(blocksArray)\n",
    "\n",
    "        histograms = []\n",
    "        # For each block and its corresponding text mask block, compute texture features\n",
    "        for block, mask_text_block in zip(blocksArray, blocksMasks):\n",
    "            \n",
    "            details = pywt.dwt2(block, 'bior1.3')\n",
    "            approx, (h, v, d) = details # approx captures bigger details (more smooth than the original img), (h, v, d) capture de horizontal, vertical and diagonal \"smaller\" details\n",
    "            \n",
    "            if mask_text_block is not None:\n",
    "                # Resize the text mask to the size of the wavelet's resulting images\n",
    "                new_mask = cv2.resize(mask_text_block, approx.shape[::-1]).astype(bool) \n",
    "\n",
    "            # Create an histogram for each wavelet \"image\" and concatenate all of them\n",
    "            final_hist = []\n",
    "            for wt_img in [approx, h, v, d]:\n",
    "                hist = np.histogram(wt_img if mask_text_block is None else wt_img[new_mask != 0], bins=bins, range=(0, 256))[0]\n",
    "                final_hist.append(hist.flatten() / (wt_img.shape[0]*wt_img.shape[1]))\n",
    "            histograms.append(np.concatenate(final_hist))\n",
    "            \n",
    "        # Concatenate all histograms into a single feature vector\n",
    "        return np.concatenate(histograms)\n",
    "\n",
    "    def get_features_by_keypoints(self, gray, mode, n_features, mask):\n",
    "        \n",
    "        if mode == 'sift':\n",
    "            # SIFT Detector\n",
    "            sift = cv2.SIFT_create(nfeatures=n_features)\n",
    "            _, des = sift.detectAndCompute(gray, mask)\n",
    "\n",
    "        elif mode == 'orb':\n",
    "            # ORB Detector\n",
    "            orb = cv2.ORB_create(nfeatures=n_features)\n",
    "            _, des = orb.detectAndCompute(gray, mask)\n",
    "\n",
    "        elif mode == 'akaze':\n",
    "            thres = 0.005\n",
    "            # AKAZE Detector\n",
    "            akaze = cv2.AKAZE_create(threshold=thres)\n",
    "            _, des = akaze.detectAndCompute(gray, mask)\n",
    "\n",
    "            while des is None or des.shape[0] < n_features:\n",
    "                if str(thres)[-1] == '1': \n",
    "                    thres = thres / 2\n",
    "                else:\n",
    "                    thres /= 5\n",
    "                \n",
    "                akaze = cv2.AKAZE_create(threshold=thres)\n",
    "                _, des = akaze.detectAndCompute(gray, mask)\n",
    "\n",
    "                if thres < 1e-6:\n",
    "                    break\n",
    "\n",
    "        return des\n",
    "\n",
    "    def clean_noise(self, image, k):\n",
    "        return cv2.medianBlur(image, k)\n",
    "    \n",
    "    # Load data, calculate background and text masks (if necessary) and compute features\n",
    "    def load_data(self, level = 3, d_hist = 1, bins = 8, n_features=2048, keypoint_mode='sift', remove_background=False, remove_text=False, features_mode='color_features'):\n",
    "        # Get a list of all image file names in the folder\n",
    "        image_files = sorted(glob.glob(self.folder_path+'/*.jpg'))\n",
    "\n",
    "        # Initialize an empty list to store the processed images and masks\n",
    "        processed_features = dict()\n",
    "        masks, masks_text = [], []\n",
    "\n",
    "        # Iterate over each image file\n",
    "        for f in tqdm.tqdm(image_files):\n",
    "            \n",
    "            # Get the image id from the file name. Depending on the OS, the path separator is different\n",
    "            try:\n",
    "                img_id = int(f.split('\\\\')[-1].split('.')[0].split('_')[-1])\n",
    "            except:\n",
    "                img_id = int(f.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "\n",
    "            # Load the image in BGR format\n",
    "            image = cv2.imread(f)\n",
    "\n",
    "            # Clean noise of the image using median filter\n",
    "            image = self.clean_noise(image, k=3)\n",
    "\n",
    "            # Convert the image to grayscale\n",
    "            image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            features_rgb, features_wavelet, features_text, features_keypoints = [], [], [], []\n",
    "\n",
    "            # Remove background (there can be 2 paintings in the same image)\n",
    "            if remove_background:\n",
    "                mask_image, coordinates = self.get_mask(image_gray)\n",
    "                n_paintings = len(coordinates)\n",
    "               \n",
    "                # Remove the text from each image\n",
    "                if remove_text:\n",
    "                    coordinates = sorted(coordinates, key=lambda x: (x[0], x[1]))\n",
    "                    masks_text_i = [[None]]*n_paintings\n",
    "                    # coordinates contains the coordinates of the paintings mask in the image\n",
    "                    # We iterate over each masked painting and get the text mask for each one\n",
    "                    # We hace to recover the original coordinates for the text mask\n",
    "                    for i, (x,y,w,h) in enumerate(coordinates):\n",
    "                        x_text, y_text, x_text_max, y_text_max, text = self.get_mask_text(image_gray[y:y+h, x:x+w], name_bag=name_bag)\n",
    "                        features_text.append(text)\n",
    "                        masks_text_i[i] = [x+x_text, y+y_text, x+x_text_max, y+y_text_max]\n",
    "                    masks_text.append(masks_text_i)\n",
    "                else:\n",
    "                    masks_text.extend([[None] for _ in range(n_paintings)])\n",
    "\n",
    "            else:\n",
    "                mask_image = None\n",
    "\n",
    "                # if there is no background, the mask is the whole image\n",
    "                coordinates = [[0,0,image.shape[1],image.shape[0]]]\n",
    "                n_paintings = 1\n",
    "                if remove_text:\n",
    "                    x_text, y_text, x_text_max, y_text_max, text = self.get_mask_text(image_gray, name_bag=name_bag)\n",
    "                    features_text.append(text)\n",
    "                    masks_text.append([[x_text, y_text, x_text_max, y_text_max]])\n",
    "                else:\n",
    "                    masks_text.append([None])\n",
    "\n",
    "            masks.append(mask_image)\n",
    "        \n",
    "            for i in range(n_paintings):\n",
    "                x,y,w,h = coordinates[i]\n",
    "\n",
    "                mask_painting = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "                mask_painting[y:y+h, x:x+w] = 255\n",
    "\n",
    "                relative_mask_text = None\n",
    "                if masks_text[-1][i] is not None:\n",
    "                    relative_mask_text = [masks_text[-1][i][0]-x, masks_text[-1][i][1]-y, masks_text[-1][i][2]-x, masks_text[-1][i][3]-y]\n",
    "                    # Remove the text from the image \n",
    "                    mask_painting[masks_text[-1][i][1]:masks_text[-1][i][3], masks_text[-1][i][0]:masks_text[-1][i][2]] = 0\n",
    "\n",
    "                if features_mode == 'color_features' or features_mode == 'combined':    \n",
    "                    # Get the features of every masked image\n",
    "                    f = self.get_color_features_by_blocks(image[y:y+h, x:x+w], level, d_hist, bins, mask_text=relative_mask_text)\n",
    "                    features_rgb.append(f)\n",
    "\n",
    "                if features_mode == 'texture_features' or features_mode == 'combined':\n",
    "                    f_wavelet = self.get_texture_features_by_blocks(image_gray[y:y+h, x:x+w], level, bins, mask_text=relative_mask_text)\n",
    "                    features_wavelet.append(f_wavelet)\n",
    "\n",
    "                if features_mode == 'keypoint':\n",
    "                    f_keypoints = self.get_features_by_keypoints(image_gray, keypoint_mode, n_features, mask_painting)\n",
    "                    features_keypoints.append(f_keypoints)\n",
    "            \n",
    "            # Append the features to the dict\n",
    "            if features_mode == 'texture_features':\n",
    "                processed_features[img_id] = features_wavelet\n",
    "            \n",
    "            elif features_mode == 'text_features':\n",
    "                processed_features[img_id] = features_text\n",
    "\n",
    "            elif features_mode == 'color_features':\n",
    "                processed_features[img_id] = features_rgb\n",
    "            \n",
    "            elif features_mode == 'combined':\n",
    "                if n_paintings > 1:\n",
    "                    assert len(features_rgb) == len(features_wavelet) == len(features_text), 'The number of features must be the same for each mode!'\n",
    "                    processed_features[img_id] = [[features_rgb[i], features_wavelet[i], features_text[i]] for i in range(n_paintings)]\n",
    "                else:\n",
    "                    processed_features[img_id] = [[features_rgb, features_wavelet, features_text]]\n",
    "            \n",
    "            elif features_mode == 'keypoint':\n",
    "                processed_features[img_id] = features_keypoints\n",
    "            \n",
    "        return processed_features, masks, masks_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://github.com/benhamner/Metrics -> Metrics.Python.ml_metrics.average_precision.py\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# Copied from https://github.com/benhamner/Metrics -> Metrics.Python.ml_metrics.average_precision.py\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for a,p in zip(actual, predicted):\n",
    "        for a_i, p_i in zip(a,p):\n",
    "            result.append(apk([a_i],p_i,k))\n",
    "    return np.mean(result)\n",
    "\n",
    "# compute the histogram intersection between two feature vectors\n",
    "def histogram_intersection(hist1, hist2, normalized=False):\n",
    "    if normalized:\n",
    "        return np.sum(np.minimum(hist1, hist2)) / np.sum(np.maximum(hist1, hist2))\n",
    "    else:\n",
    "        return np.sum(np.minimum(hist1, hist2))\n",
    "\n",
    "# compute the chi-squared distance between two feature vectors\n",
    "def chi_squared_distance(hist1, hist2):\n",
    "    return np.sum(np.square(hist1 - hist2) / (hist1 + hist2 + 1e-10))\n",
    "\n",
    "# compute the euclidean distance between two feature vectors\n",
    "def euclidean_distance(hist1, hist2):\n",
    "    return np.sqrt(np.sum(np.square(hist1 - hist2)))\n",
    "\n",
    "def custom_leveshtein_distance(s1, s2, normalized=False):\n",
    "    if normalized:\n",
    "        return (max(len(s1), len(s2)) - levenshtein_distance(s1, s2)) / max(len(s1), len(s2))\n",
    "    else:\n",
    "        return levenshtein_distance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the closest k DDBB image for query images determined by the similarity function. \n",
    "# The features have been previously calculated from the developed method.\n",
    "# It returns a list of lists with the k closest images for each query image. \n",
    "def compare_images(query_features, bbdd_features, k, sim_func, param=None, filter=False, combine=False):\n",
    "    \n",
    "    result = []\n",
    "    for id1,f1 in query_features.items():\n",
    "        result_i = []\n",
    "        for f_i in f1:\n",
    "            distances = []\n",
    "            for id2,f2 in bbdd_features.items():\n",
    "                text_bd = utils.get_text_bbdd(f'data/BBDD/bbdd_{str(id2).zfill(5)}.txt')\n",
    "\n",
    "                # Use the provided similarity function for comparing both paintings\n",
    "                if not filter and not combine:\n",
    "                    distances.append((id2, sim_func(f_i,f2)))\n",
    "                    continue\n",
    "                \n",
    "                for f2_i in f2:\n",
    "                    # First filter those paintings that have the same author. Then, compute similarity/distance for retrieval\n",
    "                    if filter and text_bd == f_i[-1][0]:\n",
    "                       distances.append((id2, param*sim_func(f_i[0][0], f2_i[0][0], normalized=True) + (1-param)*sim_func(f_i[1][0], f2_i[1][0], normalized=True)))\n",
    "                    \n",
    "                    # Use weighted sum between the color, texture and text scores as the similarity score for the retrieval\n",
    "                    elif combine:\n",
    "                        # If the similarity function is a distance \n",
    "                        if sim_func in [chi_squared_distance, levenshtein_distance]:\n",
    "                            distances.append(\n",
    "                                (id2, \n",
    "                                param[0]*sim_func(f_i[0][0], f2_i[0][0], normalized=True) + \n",
    "                                param[1]*sim_func(f_i[1][0], f2_i[1][0], normalized=True) +\n",
    "                                param[2]*(1 - custom_leveshtein_distance(f_i[2][0], text_bd, normalized=True))\n",
    "                                ))\n",
    "                        else:\n",
    "                            distances.append(\n",
    "                                (id2, \n",
    "                                param[0]*sim_func(f_i[0][0], f2_i[0][0], normalized=True) + \n",
    "                                param[1]*sim_func(f_i[1][0], f2_i[1][0], normalized=True) +\n",
    "                                param[2]*custom_leveshtein_distance(f_i[2][0], text_bd, normalized=True)\n",
    "                                ))\n",
    "                \n",
    "            #get k smallest values from distances   \n",
    "            if sim_func in [chi_squared_distance, levenshtein_distance]:\n",
    "                k_smallest = sorted(distances, reverse=False, key=lambda x: x[1])[:k]\n",
    "            else:\n",
    "                k_smallest = sorted(distances, reverse=True, key=lambda x: x[1])[:k]\n",
    "            result_i.append((id1, k_smallest))\n",
    "            \n",
    "        result.append(result_i)\n",
    "    \n",
    "    # Transform the result into the required format\n",
    "    result2 = []\n",
    "    for x in result:\n",
    "        result2_i = []\n",
    "        for y in x:\n",
    "            result2_i.append([z[0] for z in y[1]])\n",
    "        result2.append(result2_i)\n",
    "    \n",
    "    return result2\n",
    "\n",
    "def compare_keypoints(features_query, features_db, k, sim_func, threshold_matches1=190):    \n",
    "    bf = cv2.BFMatcher(sim_func, crossCheck=False)\n",
    "    result = []\n",
    "    for id_q, f_query in tqdm.tqdm(features_query.items(), desc='Computing matches'):\n",
    "        result_i = []\n",
    "        for f in f_query:\n",
    "            number_matches = []\n",
    "            for id_db, f_db in features_db.items():\n",
    "                \n",
    "                matches = bf.knnMatch(f, f_db[0], k=2)\n",
    "\n",
    "                good_matches = []\n",
    "                for m, n in matches:\n",
    "                    if m.distance < 0.7 * n.distance:\n",
    "                        good_matches.append(m)\n",
    "                number_matches.append((id_db, len(good_matches)))\n",
    "\n",
    "            number_matches = sorted(number_matches, reverse=True, key=lambda x: x[1])[:k]\n",
    "\n",
    "            # If the number of matches is below a certain threshold, we consider that the query image is not in the database\n",
    "            if number_matches[0][1] < threshold_matches1 and len(f_query) == 1:\n",
    "                result_i.append((id_q, [[-1]]))\n",
    "            else:\n",
    "                result_i.append((id_q, number_matches))\n",
    "                \n",
    "        result.append(result_i)\n",
    "    \n",
    "    # Transform the result into the required format\n",
    "    result2 = []\n",
    "    for x in result:\n",
    "        result2_i = []\n",
    "        for y in x:\n",
    "            result2_i.append([z[0] for z in y[1]])\n",
    "        result2.append(result2_i)\n",
    "\n",
    "    return result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader objects for both the database and the queries\n",
    "data_loader = DataLoader('data/BBDD')\n",
    "data_loader_qsd1_w4 = DataLoader('data/qsd1_w4')\n",
    "\n",
    "# Load ground truth files for each query\n",
    "with open('data/qsd1_w4/gt_corresps.pkl', 'rb') as f:\n",
    "    gt_w4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation results\n",
    "\n",
    "### Task 1: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286/286 [04:01<00:00,  1.19it/s]\n",
      " 13%|█▎        | 4/30 [00:06<00:37,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping! Two masks in the same painting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:30<00:14,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping! Two masks in the same painting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:40<00:03,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping! Two masks in the same painting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:46<00:00,  1.56s/it]\n",
      "Computing matches: 100%|██████████| 30/30 [07:38<00:00, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sift, L2, 2048 features = mAP@5: 0.8421052631578947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "features_mode = 'keypoint'\n",
    "keypoint_mode, sim_func = 'sift', cv2.NORM_L2 \n",
    "n_features = 2048\n",
    "# Compute features for the database and the query images\n",
    "features, _, _ = data_loader.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=False)\n",
    "features_q1_w4, _, _ = data_loader_qsd1_w4.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=True, remove_text=True,)\n",
    "\n",
    "result = compare_keypoints(features_q1_w4, features, k, sim_func, threshold_matches1=190)\n",
    "mapk_1 = mapk(gt_w4, result, k)\n",
    "\n",
    "print(f'Sift, L2, {n_features} features = mAP@{k}: {mapk_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286/286 [00:59<00:00,  4.83it/s]\n",
      " 13%|█▎        | 4/30 [00:04<00:21,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping! Two masks in the same painting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:15<00:06,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping! Two masks in the same painting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:19<00:01,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping! Two masks in the same painting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:22<00:00,  1.32it/s]\n",
      "Computing matches: 100%|██████████| 30/30 [06:55<00:00, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orb, Hamming, 2048 features = mAP@5: 0.8421052631578947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "features_mode = 'keypoint'\n",
    "keypoint_mode, sim_func = 'orb', cv2.NORM_HAMMING2 \n",
    "n_features = 2048\n",
    "# Compute features for the database and the query images\n",
    "features, _, _ = data_loader.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=False)\n",
    "features_q1_w4, _, _ = data_loader_qsd1_w4.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=True, remove_text=True,)\n",
    "\n",
    "result = compare_keypoints(features_q1_w4, features, k, sim_func, threshold_matches1=100)\n",
    "mapk_1 = mapk(gt_w4, result, k)\n",
    "\n",
    "print(f'Orb, Hamming, {n_features} features = mAP@{k}: {mapk_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286/286 [10:45<00:00,  2.26s/it]\n",
      " 13%|█▎        | 4/30 [00:10<00:59,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping! Two masks in the same painting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [01:29<01:11,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping! Two masks in the same painting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [01:53<00:10,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping! Two masks in the same painting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:02<00:00,  4.09s/it]\n",
      "Computing matches:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "features_mode = 'keypoint'\n",
    "keypoint_mode, sim_func = 'akaze', cv2.NORM_HAMMING2 \n",
    "n_features = 512\n",
    "# Compute features for the database and the query images\n",
    "features, _, _ = data_loader.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=False)\n",
    "features_q1_w4, _, _ = data_loader_qsd1_w4.load_data(features_mode=features_mode, n_features=n_features, keypoint_mode=keypoint_mode, remove_background=True, remove_text=True,)\n",
    "\n",
    "result = compare_keypoints(features_q1_w4, features, k, sim_func, threshold_matches1=40)\n",
    "mapk_1 = mapk(gt_w4, result, k)\n",
    "\n",
    "print(f'Akaze, Hamming, {n_features} features = mAP@{k}: {mapk_1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
