{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1 - Content Based Image Retrieval\n",
    "### Team 8 - Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pytesseract\n",
    "import re\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Luis\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_bag = set()\n",
    "for folder in ['BBDD', 'qsd1_w3', 'qsd2_w3']:\n",
    "    for text_file in glob.glob(f'data/{folder}/*.txt'):\n",
    "        # read text file\n",
    "        with open(text_file, 'r') as f:\n",
    "            line = f.readlines()\n",
    "        \n",
    "        for l in line:\n",
    "            if re.search(r\"\\('([^']+)'\", l.split(',')[0]):\n",
    "                author = re.search(r\"\\('([^']+)'\", l.split(',')[0]).group(1)\n",
    "                name_bag.add(author)\n",
    "            else:\n",
    "                name_bag.add('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "\n",
    "    # Obtain the painting image removing the background. \n",
    "    # It returns the mask where 1 means painting image and 0 background.\n",
    "    def get_mask(self, img, mode, threshold_area=71000):\n",
    "\n",
    "        # Transforming color image to grayscale depending on the color space\n",
    "        if mode == 'rgb':\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        elif mode == 'hsv':\n",
    "            gray = img[:,:,2].copy()\n",
    "        else:\n",
    "            gray = img[:,:,0].copy()\n",
    "\n",
    "        # Empty mask definition\n",
    "        mask = np.zeros(gray.shape, dtype=np.uint8)\n",
    "\n",
    "        # Applying gaussian blurring and define an intelligent gradient threshold depending on 13x13 boxes\n",
    "        blur = cv2.GaussianBlur(gray, (13,13), 0)\n",
    "        # Threshold based on local pixel neighborhood (11x11 block size)\n",
    "        thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "        # Two pass dilate with horizontal and vertical kernel\n",
    "        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,5))\n",
    "        dilate = cv2.dilate(thresh, horizontal_kernel, iterations=2)\n",
    "        vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,1))\n",
    "        dilate = cv2.dilate(dilate, vertical_kernel, iterations=2)\n",
    "\n",
    "        # Find contours, filter using contour threshold area, and draw rectangle\n",
    "        cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "        '''\n",
    "        cv2.RETR_EXTERNAL retrieves only the extreme outer contours.\n",
    "        cv2.CHAIN_APPROX_SIMPLE compresses horizontal, vertical, and diagonal segments and leaves only their end points.\n",
    "        Follows Satoshi Suzuki's algorithm:\n",
    "            1- The algorithm works by border following, which means it traces the boundary of connected components in the image.\n",
    "            2- It starts from the top-left corner of the image and looks for the first white pixel. Once found, it begins to follow the contour border.\n",
    "            3- While following the contour, the algorithm keeps track of the direction in which it is moving to ensure it stays on the boundary.\n",
    "            4- Once the entire contour is followed and the start point is reached again, the algorithm continues scanning the image for the next contour.\n",
    "        '''\n",
    "\n",
    "        #  Filtering the found contours by size\n",
    "        counter = 0\n",
    "        areas = []\n",
    "        coordinates = []\n",
    "        for c in cnts:\n",
    "            # Shoelace formula for convex shapes\n",
    "            area = cv2.contourArea(c) \n",
    "            if area > threshold_area:\n",
    "                x,y,w,h = cv2.boundingRect(c) \n",
    "                areas.append((area, (x,y,w,h)))\n",
    "                counter += 1\n",
    "\n",
    "        # Sort areas and positions by area\n",
    "        areas = sorted(areas, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Draw bounding box on mask\n",
    "        for i in range(min(len(areas), 2)):\n",
    "            x,y,w,h = areas[i][1]\n",
    "            coordinates.append((x,y,w,h))\n",
    "            mask[y:y+h, x:x+w] = 255\n",
    "        \n",
    "        # Catching the 0 contours error\n",
    "        if counter == 0:\n",
    "            print('Error! No paintings in this image!')\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "            plt.imshow(mask, cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "        return mask, coordinates\n",
    "    \n",
    "    # Obtain the painting image removing the text. \n",
    "    # It returns the mask where 1 means painting image and 0 text.\n",
    "    def get_mask_text(self, img, return_text=False):\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Applying opening, closing and dilation morphological operations with a 9x9 and 13x13 kernel\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9,9))\n",
    "        opening = cv2.morphologyEx(gray, cv2.MORPH_OPEN, kernel)\n",
    "        closing = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        x = closing-opening\n",
    "        x = (x>125).astype(np.uint8) #thresholding to get (hopefully) only the text\n",
    "\n",
    "        kernel2 = cv2.getStructuringElement(cv2.MORPH_RECT, (13,13))\n",
    "        dilated = cv2.dilate(x, kernel2, iterations=2)\n",
    "\n",
    "        # Find contours \n",
    "        ctns = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Find rectangular shapes and get the biggest one\n",
    "        areas = []\n",
    "        for c in ctns[0]:\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            if w > h and w/h < 12 and (w*h)/(img.shape[0]*img.shape[1]) < 0.35:\n",
    "                # Shoelace formula for convex shapes\n",
    "                areas.append((cv2.contourArea(c), (x,y,w,h)))\n",
    "        areas = sorted(areas, key=lambda x: x[0], reverse=True)\n",
    "        x, y, w, h = areas[0][1]\n",
    "\n",
    "\n",
    "        # If there is a shape on the right or left of the biggest shape found, we join it to the mask\n",
    "        for _, shape in areas:\n",
    "            if y > shape[1]-10 and y < shape[1]+10:\n",
    "                if shape[0] < x:\n",
    "                    w = (x+w) - shape[0]\n",
    "                    x = shape[0]\n",
    "                else:\n",
    "                    w = (shape[0]+shape[2]) - x\n",
    "\n",
    "        text = None\n",
    "        if return_text:\n",
    "            binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "            text = pytesseract.image_to_string(binary[y:y+h, x:x+w])\n",
    "            text = re.sub(r'[0-9\\n¥“«!|]', '', text)\n",
    "\n",
    "            min_dist = 1000000\n",
    "            for name in name_bag:\n",
    "                dist = levenshtein_distance(text, name)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_word = name\n",
    "\n",
    "            text = min_word\n",
    "            # print(text, min_dist)\n",
    "        \n",
    "        return [x, y, x+w, y+h, text]\n",
    "\n",
    "    # Divide the image into blocks\n",
    "    def create_blocks_array(self, image, blockNumber):\n",
    "    \n",
    "        # Set number of slices per axis\n",
    "        axisSlice = int(math.sqrt(blockNumber))\n",
    "\n",
    "        blocksArray = []\n",
    "        # Split the image into vertical blocks\n",
    "        split_h = np.array_split(image, axisSlice, axis = 0)\n",
    "        \n",
    "        for i in range(axisSlice):\n",
    "            for j in range(axisSlice):\n",
    "                # Split vertical blocks into square blocks\n",
    "                split_hv = np.array_split(split_h[i], axisSlice, axis = 1)\n",
    "                blocksArray.append(split_hv[j])\n",
    "        return blocksArray\n",
    "\n",
    "    # Compute the histogram of the image\n",
    "    def create_histogram(self, block, mask, d_hist, mode, bins):\n",
    "        channels = cv2.split(block)\n",
    "\n",
    "        if mode == 'lab' and d_hist < 3: channels = channels[1:]\n",
    "        \n",
    "        range_a, range_b = 256, 256\n",
    "        if mode == 'hsv': range_a = 180\n",
    "\n",
    "        if d_hist == 1:\n",
    "            if mask is None:\n",
    "                # Compute 1D histograms for each channel separately\n",
    "                hist = [cv2.calcHist([chan], [0], None, [bins], [0, range_a if i == 0 else range_b]) for i,chan in enumerate(channels)]\n",
    "            else:\n",
    "                # Compute 1D histograms for each channel separately\n",
    "                hist = [cv2.calcHist([chan[mask!=0]], [0], None, [bins], [0, range_a if i == 0 else range_b]) for i,chan in enumerate(channels)]\n",
    "\n",
    "        elif d_hist == 2:\n",
    "            if mask is None:\n",
    "                # Compute 2D joint histograms for each pair of channels\n",
    "                hist = [cv2.calcHist([channels[i], channels[j]], [0, 1], None, [bins, bins], [0, range_a if i == 0 else range_b, 0, range_b])\n",
    "                            for i in range(len(channels)) for j in range(i+1, len(channels))]\n",
    "            else:\n",
    "                # Compute 2D joint histograms for each pair of channels\n",
    "                hist = [cv2.calcHist([channels[i][mask!=0], channels[j][mask!=0]], [0, 1], None, [bins, bins], [0, range_a if i == 0 else range_b, 0, range_b])\n",
    "                            for i in range(len(channels)) for j in range(i+1, len(channels))]\n",
    "\n",
    "        else:\n",
    "            if mask is None:\n",
    "                # Compute 3D joint histogram for all three channels\n",
    "                hist, _ = np.histogramdd([c.flatten() for c in channels], bins=(bins, bins, bins), range=[(0, range_a), (0, range_b), (0, range_b)])\n",
    "            else:\n",
    "                # Compute 3D joint histogram for all three channels\n",
    "                hist, _ = np.histogramdd([c[mask != 0] for c in channels], bins=(bins, bins, bins), range=[(0, range_a), (0, range_b), (0, range_b)])\n",
    "\n",
    "        return hist\n",
    "    \n",
    "    # Compute the histogram of the image by blocks\n",
    "    def get_features_by_blocks(self, image, level, mode, d_hist, bins, mask_text):\n",
    "\n",
    "        # Get blocks using multi-level resolution\n",
    "        blocksArray = []\n",
    "        for lvl in range(level+1):\n",
    "            for b in self.create_blocks_array(image, (2**lvl)*(2**lvl)):\n",
    "                blocksArray.append(b)\n",
    "\n",
    "        if mask_text is not None:\n",
    "            blocksMasks = []\n",
    "\n",
    "            # We create a mask image blocking the bbox of the text\n",
    "            # That image will be used to compute the histogram of the image without the text\n",
    "            mask_text_image = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "            assert len(mask_text_image.shape) == 2, 'Mask must be a grayscale image'\n",
    "\n",
    "            mask_text_image[mask_text[1]:mask_text[3], mask_text[0]:mask_text[2]] = 0\n",
    "\n",
    "            # It is necessary to create the blocks of the mask image too\n",
    "            for lvl in range(level+1):\n",
    "                for b in self.create_blocks_array(mask_text_image, (2**lvl)*(2**lvl)):\n",
    "                    blocksMasks.append(b)\n",
    "        else:\n",
    "            blocksMasks = [None]*len(blocksArray)\n",
    "\n",
    "        histograms = []\n",
    "        for block, mask_text_block in zip(blocksArray, blocksMasks):\n",
    "            # Compute the histogram of the channel and append it to the list\n",
    "            hist = self.create_histogram(block, mask_text_block, d_hist, mode, bins)\n",
    "            if isinstance(hist, list):\n",
    "                for h in hist:\n",
    "                    histograms.append(h.flatten() / (block.shape[0]*block.shape[1]))\n",
    "            else:\n",
    "                histograms.append(hist.flatten()  / (block.shape[0]*block.shape[1]))\n",
    "            \n",
    "        # Concatenate all histograms into a single feature vector\n",
    "        return np.concatenate(histograms)\n",
    "\n",
    "    def clean_noise(self, image, k):\n",
    "        return cv2.medianBlur(image, k)\n",
    "    \n",
    "    # Load data, calculate background and text masks (if necessary) and compute features\n",
    "    def load_data(self, level = 3, d_hist = 1, bins = 64, remove_background=False, remove_text=False, return_text=False, features_mode='color_features'):\n",
    "        # Get a list of all image file names in the folder\n",
    "        image_files = sorted(glob.glob(self.folder_path+'/*.jpg'))\n",
    "\n",
    "        # Initialize an empty list to store the processed images and masks\n",
    "        processed_features_rgb = dict()\n",
    "        processed_features_hsv = dict()\n",
    "        processed_features_lab = dict()\n",
    "        masks, masks_text = [], []\n",
    "\n",
    "        # Iterate over each image file\n",
    "        for f in tqdm.tqdm(image_files):\n",
    "            \n",
    "            # Get the image id from the file name. Depending on the OS, the path separator is different\n",
    "            try:\n",
    "                img_id = int(f.split('\\\\')[-1].split('.')[0].split('_')[-1])\n",
    "            except:\n",
    "                img_id = int(f.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "\n",
    "            # Load the image\n",
    "            image = cv2.imread(f)\n",
    "            # Convert the image from BGR to lab color space\n",
    "            image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
    "            # Convert the image from BGR to HSV color space\n",
    "            image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            image, image_lab, image_hsv = self.clean_noise(image, k=3), self.clean_noise(image_lab, k=3), self.clean_noise(image_hsv, k=3)\n",
    "\n",
    "            # Remove background (there can be 2 paintings in the same image)\n",
    "            if remove_background:\n",
    "                mask_image, coordinates = self.get_mask(image, 'rgb')\n",
    "               \n",
    "                # Remove the text from each image\n",
    "                if remove_text:\n",
    "                    coordinates = sorted(coordinates, key=lambda x: (x[0], x[1]))\n",
    "                    masks_text_i = [[None]]*len(coordinates)\n",
    "\n",
    "                    # coordinates contains the coordinates of the paintings mask in the image\n",
    "                    # We iterate over each masked painting and get the text mask for each one\n",
    "                    # We hace to recover the original coordinates for the text mask\n",
    "                    for i, (x,y,w,h) in enumerate(coordinates):\n",
    "                        x_text, y_text, w_text, h_text, text = self.get_mask_text(image[y:y+h, x:x+w], return_text=return_text)\n",
    "                        masks_text_i[i] = [x+x_text, y+y_text, x+x_text+w_text, y+y_text+h_text]\n",
    "                    masks_text.append(masks_text_i)\n",
    "                else:\n",
    "                    for i in range(len(coordinates)):\n",
    "                        masks_text.append([None])\n",
    "\n",
    "            else:\n",
    "                mask_image = None\n",
    "\n",
    "                # if there is no background, the mask is the whole image\n",
    "                coordinates = [[0,0,image.shape[1],image.shape[0]]] \n",
    "                if remove_text:\n",
    "                    x_text, y_text, w_text, h_text, text = self.get_mask_text(image, return_text=return_text)\n",
    "                    masks_text.append([[x_text, y_text, w_text, h_text]])\n",
    "                else:\n",
    "                    masks_text.append([None])\n",
    "\n",
    "            masks.append(mask_image)\n",
    "            \n",
    "            features, features_hsv, features_lab = [], [], []\n",
    "            for i in range(len(coordinates)):\n",
    "                x,y,w,h = coordinates[i]\n",
    "\n",
    "                relative_mask_text = None\n",
    "                if masks_text[-1][i] is not None:\n",
    "                    relative_mask_text = [masks_text[-1][i][0]-x, masks_text[-1][i][1]-y, masks_text[-1][i][2]-x, masks_text[-1][i][3]-y]\n",
    "\n",
    "                if features_mode == 'color_features':    \n",
    "                    # Get the features of every masked image\n",
    "                    f = self.get_features_by_blocks(image[y:y+h, x:x+w], level, 'rgb', d_hist, bins, mask_text=relative_mask_text)\n",
    "                    f_hsv = self.get_features_by_blocks(image_hsv[y:y+h, x:x+w], level, 'hsv', d_hist, bins, mask_text=relative_mask_text)\n",
    "                    f_lab = self.get_features_by_blocks(image_lab[y:y+h, x:x+w], level, 'lab', d_hist, bins, mask_text=relative_mask_text)\n",
    "\n",
    "                elif features_mode == 'texture_features':\n",
    "                    ...\n",
    "                \n",
    "                elif features_mode == 'text_features':\n",
    "                    f, f_hsv, f_lab = text, text, text\n",
    "\n",
    "                features.append(f)\n",
    "                features_hsv.append(f_hsv)\n",
    "                features_lab.append(f_lab)\n",
    "\n",
    "            # Append the features to the dict\n",
    "            processed_features_rgb[img_id] = features\n",
    "            processed_features_hsv[img_id] = features_hsv\n",
    "            processed_features_lab[img_id] = features_lab\n",
    "            \n",
    "        return processed_features_rgb, processed_features_hsv, processed_features_lab, masks, masks_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://github.com/benhamner/Metrics -> Metrics.Python.ml_metrics.average_precision.py\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# Copied from https://github.com/benhamner/Metrics -> Metrics.Python.ml_metrics.average_precision.py\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for a,p in zip(actual, predicted):\n",
    "        for a_i, p_i in zip(a,p):\n",
    "            result.append(apk([a_i],p_i,k))\n",
    "    return np.mean(result)\n",
    "\n",
    "# compute the histogram intersection between two feature vectors\n",
    "def histogram_intersection(hist1, hist2):\n",
    "    return np.sum(np.minimum(hist1, hist2))\n",
    "\n",
    "# compute the euclidian distance between two feature vectors\n",
    "def euclidian_distance(hist1, hist2):\n",
    "    return np.sqrt(np.sum(np.square(hist1 - hist2)))\n",
    "\n",
    "# compute the chi-squared distance between two feature vectors\n",
    "def chi_squared_distance(hist1, hist2):\n",
    "    return np.sum(np.square(hist1 - hist2) / (hist1 + hist2 + 1e-10))\n",
    "\n",
    "# compute the bhattacharyya distance between two feature vectors\n",
    "def bhattacharyya_distance(hist1, hist2):\n",
    "    # Ensure that both histograms have the same shape\n",
    "    assert hist1.shape == hist2.shape, \"Histograms must have the same shape\"\n",
    "    # Calculate the Bhattacharyya coefficient\n",
    "    bhattacharyya_coeff = np.sum(np.sqrt(hist1 * hist2))\n",
    "    # Calculate the Bhattacharyya distance\n",
    "    bhattacharyya_distance = -np.log(bhattacharyya_coeff)\n",
    "    return bhattacharyya_distance\n",
    "\n",
    "# compute the Helling distance (Hellinger kernel) between two feature vectors\n",
    "def hellinger_kernel(hist1, hist2):\n",
    "    return np.sum(np.sqrt(hist1*hist2))\n",
    "\n",
    "# based on the solution in https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation by @Martin Thoma\n",
    "def get_iou(bb1, bb2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bb1 : list ['x1', 'x2', 'y1', 'y2']\n",
    "        The (x1, y1) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "    bb2 : list ['x1', 'x2', 'y1', 'y2']\n",
    "        The (x1, y1) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        in [0, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1[0], bb2[0])\n",
    "    y_top = max(bb1[1], bb2[1])\n",
    "    x_right = min(bb1[2], bb2[2])\n",
    "    y_bottom = min(bb1[3], bb2[3])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the closest k DDBB image for query images determined by the similarity function. \n",
    "# The features have been previously calculated from the developed method.\n",
    "# It returns a list of lists with the k closest images for each query image. \n",
    "def compare_images(query_features, bbdd_features, k, sim_func):\n",
    "    \n",
    "    result = []\n",
    "    for id1,f1 in query_features.items():\n",
    "        result_i = []\n",
    "        for i,f_i in enumerate(f1):\n",
    "            distances = []\n",
    "            for id2,f2 in bbdd_features.items():\n",
    "                distances.append((id2, sim_func(f_i,f2)))\n",
    "                #get k smallest values from distances\n",
    "                \n",
    "            if sim_func in [euclidian_distance, chi_squared_distance, bhattacharyya_distance, levenshtein_distance]:\n",
    "                k_smallest = sorted(distances, reverse=False, key=lambda x: x[1])[:k]\n",
    "            else:\n",
    "                k_smallest = sorted(distances, reverse=True, key=lambda x: x[1])[:k]\n",
    "            result_i.append((id1, k_smallest))\n",
    "            \n",
    "        result.append(result_i)\n",
    "        \n",
    "    result2 = []\n",
    "    for x in result:\n",
    "        result2_i = []\n",
    "        for y in x:\n",
    "            result2_i.append([z[0] for z in y[1]])\n",
    "        result2.append(result2_i)\n",
    "    \n",
    "    return result2\n",
    "\n",
    "# Calculate Intersection over Union (IoU) for each query image and plot the results (if necessary).\n",
    "def calculate_iou(gt_bboxes, pred_bboxes, do_plot=False):\n",
    "    plot_rows, plot_cols = 4, 4\n",
    "\n",
    "    # Plot config\n",
    "    if do_plot:\n",
    "        fig, ax = plt.subplots(nrows=plot_rows, ncols=plot_cols, figsize=(20, 20))\n",
    "\n",
    "    # Get mean IoU score\n",
    "    results = []\n",
    "    for i, (q_gt, q_pred) in enumerate(zip(gt_bboxes, pred_bboxes)):\n",
    "        for bbox_gt, bbox_pred in zip(q_gt, q_pred):\n",
    "            bbox_gt_new = [bbox_gt[0][0], bbox_gt[0][1], bbox_gt[2][0], bbox_gt[2][1]]\n",
    "            results.append(get_iou(bbox_pred, bbox_gt_new))\n",
    "\n",
    "        if i < plot_rows*plot_cols and do_plot:\n",
    "            im = cv2.imread(f\"data/qsd1_w2/{str(i).zfill(5)}.jpg\")\n",
    "            plt_idx = np.unravel_index(i, (plot_rows, plot_cols))\n",
    "            ax[plt_idx].imshow(im)\n",
    "            ax[plt_idx].add_patch(plt.Rectangle(\n",
    "                (bbox_gt_new[0], bbox_gt_new[1]), \n",
    "                bbox_gt_new[2]- bbox_gt_new[0],\n",
    "                bbox_gt_new[3]- bbox_gt_new[1],\n",
    "                edgecolor=\"green\", facecolor=\"none\", lw=2))\n",
    "            ax[plt_idx].add_patch(plt.Rectangle(\n",
    "                (bbox_pred[0], bbox_pred[1]), \n",
    "                bbox_pred[2]- bbox_pred[0],\n",
    "                bbox_pred[3]- bbox_pred[1],\n",
    "                edgecolor=\"red\", facecolor=\"none\", lw=2))\n",
    "    if do_plot: plt.show()\n",
    "\n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader objects for both the database and the queries\n",
    "data_loader = DataLoader('data/BBDD')\n",
    "data_loader_qsd1_w3 = DataLoader('data/qsd1_w3')\n",
    "data_loader_qsd2_w3 = DataLoader('data/qsd2_w3')\n",
    "\n",
    "# Load ground truth files for each query\n",
    "with open('data/qsd1_w3/gt_corresps.pkl', 'rb') as f:\n",
    "    gt_w3_1 = pickle.load(f)\n",
    "\n",
    "with open('data/qsd2_w3/gt_corresps.pkl', 'rb') as f:\n",
    "    gt_w3_2 = pickle.load(f)\n",
    "\n",
    "# Load ground truths for the text bboxes in qsd1_w3\n",
    "with open('data/qsd1_w3/text_boxes.pkl', 'rb') as f:\n",
    "    bboxes_gt_d1w3 = pickle.load(f)\n",
    "\n",
    "# Load ground truths for the text bboxes in qsd2_w3\n",
    "with open('data/qsd2_w3/text_boxes.pkl', 'rb') as f:\n",
    "    bboxes_gt_d2w3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation results\n",
    "\n",
    "### Task 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:16<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@5 for qsd1_w3: 0.46222222222222226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define best hyperparameters configuration\n",
    "features_mode = 'text_features'\n",
    "k = 5\n",
    "\n",
    "# Calculate and store the features\n",
    "bbdd_text = dict()\n",
    "for i, text_file in enumerate(glob.glob(f'data/BBDD/*.txt')):\n",
    "    # read text file\n",
    "    with open(text_file, 'r') as f:\n",
    "        line = f.readlines()\n",
    "    \n",
    "    for l in line:\n",
    "        if re.search(r\"\\('([^']+)'\", l.split(',')[0]):\n",
    "            author = re.search(r\"\\('([^']+)'\", l.split(',')[0]).group(1)\n",
    "            bbdd_text[i] = author\n",
    "\n",
    "query_text, _ , _, _, _ = data_loader_qsd1_w3.load_data(remove_text=True, return_text=True, features_mode=features_mode)\n",
    "\n",
    "results = compare_images(query_text, bbdd_text, k, levenshtein_distance)\n",
    "mapk_1 = mapk(gt_w3_1, results, k)\n",
    "print(f'MAP@{k} for qsd1_w3: {mapk_1}')    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Modest Cuixart'] -> ('Modest Cuixart'\n",
      "['Joan Ponc'] -> ('Joan Ponc'\n",
      "['Per Krohg'] -> ('Per Krohg'\n",
      "['Pere Santilari'] -> ('Pere Santilari'\n",
      "['Unknown'] -> ('Antoni Clave'\n",
      "['Agusti Puig'] -> ('Agusti Puig'\n",
      "['Edvard Munch'] -> ('Edvard Munch'\n",
      "['Jose M. Codina'] -> ('Jose M. Codina'\n",
      "['Qi Hao'] -> ('Joan Hernandez Pijuan'\n",
      "['Alfred Figueras'] -> ('Alfred Figueras'\n",
      "['Francesc Artigau'] -> ('Francesc Artigau'\n",
      "['Gerard Sala'] -> ('Gerard Sala'\n",
      "['Antoni Llena'] -> ('Antoni Llena'\n",
      "['Pere Santilari'] -> ('Pere Santilari'\n",
      "['Joan Ponc'] -> ('Joan Ponc'\n",
      "['Josep Guinovart'] -> ('Josep Guinovart'\n",
      "['Joan Ponc'] -> ('Joan Ponc'\n",
      "['Anders Svarstad'] -> ('Anders Svarstad'\n",
      "['Qi Hao'] -> ('Per Krohg'\n",
      "['Joan Pere Viladecans'] -> ('Joan Pere Viladecans'\n",
      "['Qi Hao'] -> ('Leticia Feduchi'\n",
      "['Qi Hao'] -> ('Sergi Barnils'\n",
      "['Xevi Vilaro'] -> ('Xevi Vilaro'\n",
      "['Yago Hortal'] -> ('Yago Hortal'\n",
      "['Sergi Barnils'] -> ('Sergi Barnils'\n",
      "['Joan Pere Viladecans'] -> ('Joan Pere Viladecans'\n",
      "['Josep Cisquella'] -> ('Josep Cisquella'\n",
      "['Francesc Artigau'] -> ('Francesc Artigau'\n",
      "['Modest Cuixart'] -> ('Modest Cuixart'\n",
      "['Perejaume'] -> ('Perejaume'\n"
     ]
    }
   ],
   "source": [
    "# THE SYSTEM IS ALMOST PERFECT BUT ONLY WITH THE AUTHOR NAME THERE IS AMBIGUITY\n",
    "lines = []\n",
    "for i in gt_w3_1:\n",
    "    with open(f'data/BBDD/bbdd_{str(i[0]).zfill(5)}.txt', 'r') as f:\n",
    "        line = f.readlines()\n",
    "        lines.append(line[0].split(',')[0])\n",
    "\n",
    "for w1,w2 in zip(lines, query_text.values()):\n",
    "    print(w2, '->', w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB, chi_squared_distance = \tmAP@1: 0.7777777777777778\n",
      "HSV, chi_squared_distance = \tmAP@1: 0.7833333333333333\n",
      "LAB, chi_squared_distance = \tmAP@1: 0.525\n",
      "RGB, histogram_intersection = \tmAP@1: 0.7583333333333333\n",
      "HSV, histogram_intersection = \tmAP@1: 0.7916666666666666\n",
      "LAB, histogram_intersection = \tmAP@1: 0.4888888888888889\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "features_mode = 'color_features'\n",
    "# Compute features for the database and the query images\n",
    "features_rgb, features_hsv, features_lab, _, _ = data_loader.load_data(features_mode=features_mode, remove_background=False, level=2, d_hist=2, bins=8)\n",
    "features_rgb_q1_w2, features_hsv_q1_w2, features_lab_q1_w2, _, _ = data_loader_qsd1_w3.load_data(features_mode=features_mode, remove_background=False, remove_text=True, level=2, d_hist=2, bins=8)\n",
    "\n",
    "# Query 1: Results and mAP@k\n",
    "for sim_func in [chi_squared_distance, histogram_intersection]:\n",
    "\n",
    "    results_rgb_q1_w3 = compare_images(features_rgb_q1_w2, features_rgb, k, sim_func)\n",
    "    results_hsv_q1_w3 = compare_images(features_hsv_q1_w2, features_hsv, k, sim_func)\n",
    "    results_lab_q1_w3 = compare_images(features_lab_q1_w2, features_lab, k, sim_func)\n",
    "\n",
    "    mapk_rgb_1 = mapk(gt_w3_1, results_rgb_q1_w3, k)\n",
    "    mapk_hsv_1 = mapk(gt_w3_1, results_hsv_q1_w3, k)\n",
    "    mapk_lab_1 = mapk(gt_w3_1, results_lab_q1_w3, k)\n",
    "\n",
    "    print(f'RGB, {sim_func.__name__} = \\tmAP@1: {mapk_rgb_1}')\n",
    "    print(f'HSV, {sim_func.__name__} = \\tmAP@1: {mapk_hsv_1}')\n",
    "    print(f'LAB, {sim_func.__name__} = \\tmAP@1: {mapk_lab_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:\n",
    "\n",
    "- Grayscale image\n",
    "- [Closing - opening] filter, both using 9x9 structuring element\n",
    "- Binarization \n",
    "- Two dilations using 13x13 structuring element\n",
    "- Bounding boxes extraction using _findContours_\n",
    "- Biggest contour represents one of the text words. We add the bounding boxes with similar height located on the right or left to the main one (if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 17.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create text bounding boxes\n",
    "_, _, _, _, masks_text = data_loader_qsd1_w2.load_data(remove_background=False, remove_text=True, level=level, d_hist=d_hist, bins=bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU: 0.45547641682825424\n"
     ]
    }
   ],
   "source": [
    "iou = calculate_iou(bboxes_gt_d1w2, masks_text)\n",
    "        \n",
    "print(\"Mean IoU:\", iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 287/287 [00:28<00:00, 10.11it/s]\n",
      "100%|██████████| 30/30 [00:01<00:00, 16.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB, chi_squared_distance = \tmAP@1: 0.4666666666666667\n",
      "HSV, chi_squared_distance = \tmAP@1: 0.5\n",
      "LAB, chi_squared_distance = \tmAP@1: 0.4\n",
      "RGB, histogram_intersection = \tmAP@1: 0.4666666666666667\n",
      "HSV, histogram_intersection = \tmAP@1: 0.4666666666666667\n",
      "LAB, histogram_intersection = \tmAP@1: 0.36666666666666664\n"
     ]
    }
   ],
   "source": [
    "# Compute features for the database and the query images\n",
    "features_rgb, features_hsv, features_lab, _, _ = data_loader.load_data(remove_background=False, level=level, d_hist=d_hist, bins=bins)\n",
    "features_rgb_q1_w2, features_hsv_q1_w2, features_lab_q1_w2, _, _ = data_loader_qsd1_w2.load_data(remove_background=False, remove_text=True, level=level, d_hist=d_hist, bins=bins)\n",
    "\n",
    "# Query 1: Results and mAP@k\n",
    "for sim_func in [chi_squared_distance, histogram_intersection]:\n",
    "\n",
    "    results_rgb_q1_w2 = compare_images(features_rgb_q1_w2, features_rgb, k, sim_func)\n",
    "    results_hsv_q1_w2 = compare_images(features_hsv_q1_w2, features_hsv, k, sim_func)\n",
    "    results_lab_q1_w2 = compare_images(features_lab_q1_w2, features_lab, k, sim_func)\n",
    "\n",
    "    mapk_rgb_1 = mapk(gt_w2_1, results_rgb_q1_w2, k)\n",
    "    mapk_hsv_1 = mapk(gt_w2_1, results_hsv_q1_w2, k)\n",
    "    mapk_lab_1 = mapk(gt_w2_1, results_lab_q1_w2, k)\n",
    "\n",
    "    print(f'RGB, {sim_func.__name__} = \\tmAP@1: {mapk_rgb_1}')\n",
    "    print(f'HSV, {sim_func.__name__} = \\tmAP@1: {mapk_hsv_1}')\n",
    "    print(f'LAB, {sim_func.__name__} = \\tmAP@1: {mapk_lab_1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6:\n",
    "\n",
    "- Grayscale image\n",
    "- Apply gaussian blur\n",
    "- Binarization using intelligent threshold based on local pixel neighborhood (11x11 block size)\n",
    "- Twice vertical and horizontal dilations, 5x1 and 1x5 structuring element respectively\n",
    "- Bounding boxes straction using _findContours_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 287/287 [00:28<00:00, 10.00it/s]\n",
      "100%|██████████| 30/30 [00:05<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB, chi_squared_distance = \tmAP@1: 0.5740740740740741\n",
      "HSV, chi_squared_distance = \tmAP@1: 0.5740740740740741\n",
      "LAB, chi_squared_distance = \tmAP@1: 0.4074074074074074\n",
      "RGB, histogram_intersection = \tmAP@1: 0.5740740740740741\n",
      "HSV, histogram_intersection = \tmAP@1: 0.5740740740740741\n",
      "LAB, histogram_intersection = \tmAP@1: 0.4074074074074074\n"
     ]
    }
   ],
   "source": [
    "# Compute features for the database and the query images\n",
    "features_rgb, features_hsv, features_lab, _, _ = data_loader.load_data(remove_background=False, level=level, d_hist=d_hist, bins=bins)\n",
    "features_rgb_q2_w2, features_hsv_q2_w2, features_lab_q2_w2, _, _ = data_loader_qsd2_w2.load_data(remove_background=True, remove_text=True, level=level, d_hist=d_hist, bins=bins)\n",
    "\n",
    "# Query 1: Results and mAP@k\n",
    "for sim_func in [chi_squared_distance, histogram_intersection]:\n",
    "\n",
    "    results_rgb_q1_w2 = compare_images(features_rgb_q2_w2, features_rgb, k, chi_squared_distance)\n",
    "    results_hsv_q1_w2 = compare_images(features_hsv_q2_w2, features_hsv, k, chi_squared_distance)\n",
    "    results_lab_q1_w2 = compare_images(features_lab_q2_w2, features_lab, k, chi_squared_distance)\n",
    "\n",
    "    mapk_rgb_1 = mapk(gt_w2_2, results_rgb_q1_w2, k)\n",
    "    mapk_hsv_1 = mapk(gt_w2_2, results_hsv_q1_w2, k)\n",
    "    mapk_lab_1 = mapk(gt_w2_2, results_lab_q1_w2, k)\n",
    "\n",
    "    print(f'RGB, {sim_func.__name__} = \\tmAP@1: {mapk_rgb_1}')\n",
    "    print(f'HSV, {sim_func.__name__} = \\tmAP@1: {mapk_hsv_1}')\n",
    "    print(f'LAB, {sim_func.__name__} = \\tmAP@1: {mapk_lab_1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
